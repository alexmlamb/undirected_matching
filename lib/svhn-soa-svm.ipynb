{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5105 on context None\n",
      "Mapped name None to device cuda0: Quadro K6000 (0000:04:00.0)\n",
      "/Tmp/lisa/os_v5/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python \n",
    "\n",
    "'''\n",
    "-Initially make z_to_x and x_to_z fairly shallow networks.  Inject noise?  \n",
    "\n",
    "-Use the fflayer class?  \n",
    "\n",
    "'''\n",
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "sys.path.append(\"/u/lambalex/DeepLearning/undirected_matching\")\n",
    "sys.path.append(\"/u/lambalex/DeepLearning/undirected_matching/lib\")\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from nn_layers import fflayer, param_init_fflayer, param_init_convlayer, convlayer\n",
    "from utils import init_tparams, join2, srng, dropout, inverse_sigmoid, join3, merge_images\n",
    "from loss import accuracy, crossent, lsgan_loss, wgan_loss, improvement_loss\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "import random\n",
    "from viz import plot_images\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "import os\n",
    "slurm_name = os.environ[\"SLURM_JOB_ID\"]\n",
    "\n",
    "class ConsiderConstant(theano.compile.ViewOp):\n",
    "    def grad(self, args, g_outs):\n",
    "        return [T.zeros_like(g_out) for g_out in g_outs]\n",
    "\n",
    "consider_constant = ConsiderConstant()\n",
    "\n",
    "\n",
    "\n",
    "def init_gparams(p):\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='z_x_1',nin=nl*2,nout=512*4*4,ortho=False,batch_norm=True)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_2',nin=512,nout=256,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_3',nin=256*1,nout=128,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_4',nin=128*1,nout=3,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_1',nin=3,nout=32,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_2',nin=32,nout=64,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_3',nin=64,nout=128,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_4',nin=128,nout=256,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_5',nin=256,nout=512,kernel_len=5,batch_norm=True)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_fc1',nin=512*4*4,nout=1024,ortho=False,batch_norm=True)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_fc2',nin=1024,nout=1024,ortho=False,batch_norm=True)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_mu',nin=1024,nout=nl,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_sigma',nin=1024,nout=nl,ortho=False,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "def init_cparams(p):\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='c_1',nin=nl+512*4*4,nout=512,ortho=False,batch_norm=True)\n",
    "    print \"mlp on top, 512 dim\"\n",
    "    p = param_init_fflayer(options={},params=p,prefix='c_2',nin=512,nout=512,ortho=False,batch_norm=True)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='c_3',nin=512,nout=10,ortho=False,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "def init_dparams(p):\n",
    "\n",
    "    print \"NOT trying batch norm in the discriminator part that sees x!\"\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_1',nin=3,nout=32,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_2',nin=32,nout=64,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_3',nin=64,nout=128,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_4',nin=128,nout=256,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_5',nin=256,nout=512,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_1',nin=nl+512*4*4,nout=nfd,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_2',nin=nfd,nout=nfd,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_3',nin=nfd,nout=nfd,ortho=False,batch_norm=False)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_1',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_2',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_3',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_4',nin=128,nout=1,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_5',nin=256,nout=1,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_6',nin=512,nout=1,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "\n",
    "def z_to_x(p,z):\n",
    "\n",
    "    print \"no extra noise input\"\n",
    "    z_inp = join2(z, 0.0*srng.normal(size=z.shape))\n",
    "\n",
    "    d0 = fflayer(tparams=p,state_below=z_inp,options={},prefix='z_x_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    d0 = d0.reshape((64,512,4,4))\n",
    "\n",
    "    d1 = convlayer(tparams=p,state_below=d0,options={},prefix='z_x_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=-2)\n",
    "\n",
    "    d2 = convlayer(tparams=p,state_below=d1,options={},prefix='z_x_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=-2)\n",
    "\n",
    "    d3 = convlayer(tparams=p,state_below=d2,options={},prefix='z_x_4',activ='lambda x: x',stride=-2)\n",
    "\n",
    "    x_new = d3.flatten(2)\n",
    "\n",
    "    return x_new\n",
    "\n",
    "def x_to_z(p,x):\n",
    "\n",
    "    e1 = convlayer(tparams=p,state_below=x.reshape((64,3,32,32)),options={},prefix='x_z_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=1)\n",
    "\n",
    "    e2 = convlayer(tparams=p,state_below=e1,options={},prefix='x_z_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    e3 = convlayer(tparams=p,state_below=e2,options={},prefix='x_z_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=1)\n",
    "\n",
    "    e4 = convlayer(tparams=p,state_below=e3,options={},prefix='x_z_4',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    e5 = convlayer(tparams=p,state_below=e4,options={},prefix='x_z_5',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    eo = e5\n",
    "    eo = eo.flatten(2)\n",
    "\n",
    "    encoder_features = eo\n",
    "\n",
    "    h1 = fflayer(tparams=p,state_below=eo,options={},prefix='x_z_fc1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "    h2 = fflayer(tparams=p,state_below=h1,options={},prefix='x_z_fc2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    sigma = fflayer(tparams=p,state_below=h2,options={},prefix='x_z_mu',activ='lambda x: x')\n",
    "    mu = fflayer(tparams=p,state_below=h2,options={},prefix='x_z_sigma',activ='lambda x: x')\n",
    "\n",
    "    eps = srng.normal(size=sigma.shape)\n",
    "\n",
    "    z_new = eps*T.nnet.sigmoid(sigma) + mu\n",
    "    print \"turned on injected noise in x->z connection\"\n",
    "\n",
    "    z_new = (z_new - T.mean(z_new, axis=0, keepdims=True)) / (0.001 + T.std(z_new, axis=0, keepdims=True))\n",
    "\n",
    "    return z_new,encoder_features\n",
    "\n",
    "def classifier(p,z,true_y):\n",
    "\n",
    "    print \"turning off gradients from classifier\"\n",
    "    z = consider_constant(z)\n",
    "\n",
    "    h1 = fflayer(tparams=p,state_below=z,options={},prefix='c_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    h2 = fflayer(tparams=p,state_below=h1,options={},prefix='c_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    y_est = fflayer(tparams=p,state_below=h2,options={},prefix='c_3',activ='lambda x: x')\n",
    "\n",
    "    y_est = T.nnet.softmax(y_est)\n",
    "\n",
    "    acc = accuracy(y_est,true_y)\n",
    "    loss = crossent(y_est,true_y)\n",
    "\n",
    "    return loss,acc\n",
    "\n",
    "def discriminator(p,x,z):\n",
    "\n",
    "    dc_1 = convlayer(tparams=p,state_below=x.reshape((64,3,32,32)),options={},prefix='DC_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=1)\n",
    "\n",
    "    dc_2 = convlayer(tparams=p,state_below=dc_1,options={},prefix='DC_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    dc_3 = convlayer(tparams=p,state_below=dc_2,options={},prefix='DC_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=1)\n",
    "\n",
    "    dc_4 = convlayer(tparams=p,state_below=dc_3,options={},prefix='DC_4',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    dc_5 = convlayer(tparams=p,state_below=dc_4,options={},prefix='DC_5',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    inp = join2(z,dc_5.flatten(2))\n",
    "\n",
    "    h1 = fflayer(tparams=p,state_below=inp,options={},prefix='D_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',mean_ln=False)\n",
    "\n",
    "    h2 = fflayer(tparams=p,state_below=h1,options={},prefix='D_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)', mean_ln=False)\n",
    "\n",
    "    h3 = fflayer(tparams=p,state_below=h2,options={},prefix='D_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)', mean_ln=False)\n",
    "\n",
    "    D1 = fflayer(tparams=p,state_below=h1,options={},prefix='D_o_1',activ='lambda x: x')\n",
    "    D2 = fflayer(tparams=p,state_below=h2,options={},prefix='D_o_2',activ='lambda x: x')\n",
    "    D3 = fflayer(tparams=p,state_below=h3,options={},prefix='D_o_3',activ='lambda x: x')\n",
    "\n",
    "    D4 = convlayer(tparams=p,state_below=dc_3,options={},prefix='D_o_4',activ='lambda x: x',stride=2)\n",
    "    D5 = convlayer(tparams=p,state_below=dc_4,options={},prefix='D_o_5',activ='lambda x: x',stride=2)\n",
    "    D6 = convlayer(tparams=p,state_below=dc_5,options={},prefix='D_o_6',activ='lambda x: x',stride=2)\n",
    "\n",
    "    print \"special thing in D\"\n",
    "    return [D1,D2,D3,D4,D5,D6], [h3,dc_5.flatten(2)]\n",
    "\n",
    "def p_chain(p, z, num_iterations):\n",
    "    zlst = [z]\n",
    "    xlst = []\n",
    "\n",
    "    if num_iterations == 1:\n",
    "        \n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        #new_z = x_to_z(p, xlst[-1])\n",
    "        #zlst.append(new_z)\n",
    "\n",
    "    elif num_iterations == 3:  \n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        new_z,_ = x_to_z(p, consider_constant(xlst[-1]))\n",
    "        zlst.append(new_z)\n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        new_z,_ = x_to_z(p, consider_constant(xlst[-1]))\n",
    "        zlst.append(new_z)\n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "\n",
    "    else:\n",
    "\n",
    "        for inds in range(0,num_iterations):\n",
    "            new_x = z_to_x(p, zlst[-1])\n",
    "            xlst.append(new_x)\n",
    "            new_z,_ = x_to_z(p, xlst[-1])\n",
    "            zlst.append(new_z)\n",
    "\n",
    "\n",
    "    for j in range(len(xlst)):\n",
    "        xlst[j] = T.nnet.sigmoid(xlst[j])\n",
    "\n",
    "    return xlst, zlst\n",
    "\n",
    "def onestep_z_to_x(p,z):\n",
    "    x = T.nnet.sigmoid(z_to_x(p, z))\n",
    "    return x\n",
    "\n",
    "def onestep_x_to_z(p,x):\n",
    "    new_z,_ = x_to_z(p, inverse_sigmoid(x))\n",
    "    return new_z\n",
    "\n",
    "def q_chain(p,x,num_iterations):\n",
    "\n",
    "    xlst = [x]\n",
    "    zlst = []\n",
    "    new_z,encoder_features = x_to_z(p, inverse_sigmoid(xlst[-1]))\n",
    "    zlst.append(new_z)\n",
    "\n",
    "    return xlst, zlst,encoder_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "directory = \"network-temp/\"\n",
    "ext = \"svhn-soa-svm.p\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "def save_network(gparams,dparams, cparams,name):\n",
    "    pkl_params = (gparams,dparams, cparams)\n",
    "    out = open(directory + str(name) + ext, \"w\", 0) #bufsize=0\n",
    "    pickle.dump(pkl_params, out)\n",
    "    out.close()\n",
    "\n",
    "def load_network(name):\n",
    "    return pickle.load(open(directory + str(name) + ext, \"r\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num latent 128\n",
      "num steps 3\n",
      "latent sparse False\n",
      "improvement loss weight 0.0\n",
      "num labeled examples used 50000\n",
      "dataset svhn\n",
      "num train examples 604388\n",
      "num test examples 26032\n",
      "NOT trying batch norm in the discriminator part that sees x!\n",
      "mlp on top, 512 dim\n",
      "no extra noise input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Quadro K6000 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "special thing in D\n",
      "single disc\n",
      "special thing in D\n",
      "turning off gradients from classifier\n",
      "not using improvement objective\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n"
     ]
    }
   ],
   "source": [
    "nl = 128\n",
    "print \"num latent\", nl\n",
    "#128 works for nl\n",
    "nfg = 512\n",
    "nfd = 512\n",
    "\n",
    "\n",
    "#3\n",
    "num_steps = 3\n",
    "print \"num steps\", num_steps\n",
    "\n",
    "latent_sparse = False\n",
    "print \"latent sparse\", latent_sparse\n",
    "\n",
    "improvement_loss_weight = 0.0\n",
    "print \"improvement loss weight\", improvement_loss_weight\n",
    "\n",
    "num_labeled_examples_use = 50000\n",
    "print \"num labeled examples used\",num_labeled_examples_use\n",
    "\n",
    "#dataset = \"mnist\"\n",
    "#dataset = \"anime\"\n",
    "dataset = \"svhn\"\n",
    "print \"dataset\", dataset\n",
    "\n",
    "if dataset == \"mnist\":\n",
    "    mn = gzip.open(\"/u/lambalex/data/mnist/mnist.pkl.gz\")\n",
    "\n",
    "    train, valid, test = pickle.load(mn)\n",
    "\n",
    "    trainx,trainy = train\n",
    "\n",
    "\n",
    "    #newtx = trainx[(trainy<2) | (trainy>8)]\n",
    "    #newty = trainy[(trainy<2) | (trainy>8)]\n",
    "    #trainx = newtx\n",
    "    #trainy = newty\n",
    "\n",
    "    validx,validy = valid\n",
    "    testx, testy = test\n",
    "\n",
    "    num_examples = trainx.shape[0]\n",
    "\n",
    "    m = 784\n",
    "\n",
    "elif dataset == \"anime\":\n",
    "    from load_file import FileData, normalize, denormalize\n",
    "\n",
    "    loc = \"/u/lambalex/DeepLearning/animefaces/datafaces/danbooru-faces/\"\n",
    "\n",
    "    animeData = FileData(loc, 32, 64)\n",
    "\n",
    "    m = 32*32*3\n",
    "\n",
    "elif dataset == \"svhn\":\n",
    "\n",
    "    from load_svhn import SvhnData\n",
    "    from load_file import normalize, denormalize\n",
    "\n",
    "    svhnData = SvhnData()\n",
    "\n",
    "    num_examples = 50000\n",
    "\n",
    "\n",
    "\n",
    "gparams = init_gparams({})\n",
    "dparams = init_dparams({})\n",
    "cparams = init_cparams({})\n",
    "\n",
    "z_in = T.matrix('z_in')\n",
    "x_in = T.matrix()\n",
    "true_y = T.ivector('true_y')\n",
    "\n",
    "p_lst_x,p_lst_z = p_chain(gparams, z_in, num_steps)\n",
    "\n",
    "q_lst_x,q_lst_z,encoder_features = q_chain(gparams, x_in, num_steps)\n",
    "\n",
    "p_lst_x_long,p_lst_z_long = p_chain(gparams, z_in, 19)\n",
    "\n",
    "z_inf = q_lst_z[-1]\n",
    "\n",
    "D_p_lst_1,_ = discriminator(dparams, p_lst_x[-1], p_lst_z[-1])\n",
    "\n",
    "if False:\n",
    "    D_p_lst_2,_ = discriminator(dparams, p_lst_x[-2], p_lst_z[-2])\n",
    "    D_p_lst = D_p_lst_1 + D_p_lst_2\n",
    "    print \"double disc\"\n",
    "else:\n",
    "    D_p_lst = D_p_lst_1\n",
    "    print \"single disc\"\n",
    "\n",
    "D_q_lst,D_feat_q = discriminator(dparams, q_lst_x[-1], q_lst_z[-1])\n",
    "\n",
    "closs,cacc = classifier(cparams,join2(z_inf,encoder_features),true_y)\n",
    "\n",
    "dloss, gloss = lsgan_loss(D_q_lst, D_p_lst)\n",
    "\n",
    "print \"not using improvement objective\"\n",
    "#improvement_objective = improvement_loss_weight * improvement_loss(D_p_lst_1, D_p_lst_2)\n",
    "#gloss += improvement_objective\n",
    "\n",
    "dupdates = lasagne.updates.rmsprop(dloss, dparams.values(),0.0001)\n",
    "gloss_grads = T.grad(gloss, gparams.values(), disconnected_inputs='ignore')\n",
    "gupdates = lasagne.updates.rmsprop(gloss_grads, gparams.values(),0.0001)\n",
    "\n",
    "gcupdates = lasagne.updates.rmsprop(gloss + closs, gparams.values() + cparams.values(),0.0001)\n",
    "dcupdates = lasagne.updates.rmsprop(dloss + closs, dparams.values() + cparams.values(),0.0001)\n",
    "\n",
    "dgupdates = dupdates.copy()\n",
    "dgupdates.update(gupdates)\n",
    "\n",
    "dgcupdates = dcupdates.copy()\n",
    "dgcupdates.update(gcupdates)\n",
    "\n",
    "train_disc_gen_classifier = theano.function(inputs = [x_in, z_in,true_y], outputs=[dloss,p_lst_x[-1],p_lst_z[-1],closs,cacc], updates=dgcupdates,on_unused_input='ignore')\n",
    "\n",
    "train_disc_gen = theano.function(inputs = [x_in, z_in], outputs=[dloss,p_lst_x[-1],p_lst_z[-1]], updates=dgupdates,on_unused_input='ignore')\n",
    "\n",
    "train_disc_gen_faster = theano.function(inputs = [x_in, z_in], outputs=dloss, updates=dgupdates,on_unused_input='ignore')\n",
    "\n",
    "test_classifier = theano.function(inputs = [x_in,true_y], outputs=[closs,cacc],on_unused_input='ignore')\n",
    "\n",
    "get_zinf = theano.function([x_in], outputs=z_inf)\n",
    "#get_dfeat = theano.function([x_in], outputs=D_feat_q)\n",
    "\n",
    "#get_pchain = theano.function([z_in], outputs = p_lst_x_long)\n",
    "\n",
    "x_in = T.matrix()\n",
    "\n",
    "func_z_to_x = theano.function([z_in], outputs = onestep_z_to_x(gparams, z_in))\n",
    "func_x_to_z = theano.function([x_in], outputs = onestep_x_to_z(gparams, x_in))\n",
    "\n",
    "z_out_p = rng.normal(size=(64,nl)).astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turned on injected noise in x->z connection\n",
      "special thing in D\n"
     ]
    }
   ],
   "source": [
    "input_x = T.matrix('x')\n",
    "\n",
    "z_inf,z_feat = x_to_z(gparams, inverse_sigmoid(input_x))\n",
    "\n",
    "d_val, d_feat = discriminator(dparams, input_x, z_inf)\n",
    "\n",
    "#z_feat, d_feat[1]\n",
    "get_dz = theano.function([input_x], outputs = [d_feat[0],d_feat[1]])\n",
    "\n",
    "get_z = func_x_to_z\n",
    "\n",
    "\n",
    "#rec = theano.function([input_x], outputs = [T.nnet.sigmoid(z_to_x(gparams,x_to_z(gparams,inverse_sigmoid(input_x))[0]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_dz(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zstats = []\n",
    "dzstats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " all test data\n",
      "test accuracy 0.779938271605\n",
      "all train set\n",
      "DDDDDDZZZZZZZ starting training! using svm linear C 1.0 squared hinge\n",
      "predicting held out\n",
      "###d# 0 testacc 0.723392571885 trainacc 1.0\n",
      "ZZZZZZ starting training! using svm linear C 1.0 squared hinge\n",
      "predicting held out\n",
      "###dz# 0 testacc 0.521216054313 trainacc 0.9375\n",
      "writing stats.pkl 56.5153329372 total time to run\n",
      "writing i 1 dloss 0.486154288054 gen_x mean 0.43579 closs 0.617333173752 cacc 0.828125\n",
      "i 8 dloss 0.498861908913 gen_x mean 0.455445 closs 0.533334970474 cacc 0.828125\n",
      "i 11 dloss 0.534040868282 gen_x mean 0.450107 closs 0.58036583662 cacc 0.8125\n",
      "i 14 dloss 0.544250667095 gen_x mean 0.446283 closs 0.502804577351 cacc 0.828125\n",
      "i 20 dloss 0.515501022339 gen_x mean 0.445165 closs 0.49504250288 cacc 0.84375\n",
      "i 28 dloss 0.502151370049 gen_x mean 0.425534 closs 0.563003778458 cacc 0.8125\n",
      "i 30 dloss 0.49825617671 gen_x mean 0.413668 closs 0.530898869038 cacc 0.828125\n",
      "i 63 dloss 0.514207243919 gen_x mean 0.423182 closs 0.545304000378 cacc 0.8125\n",
      "i 67 dloss 0.513800740242 gen_x mean 0.420827 closs 0.635829806328 cacc 0.8125\n",
      "i 72 dloss 0.498342931271 gen_x mean 0.42863 closs 0.511046051979 cacc 0.84375\n",
      "i 75 dloss 0.47821983695 gen_x mean 0.442548 closs 0.418915838003 cacc 0.875\n",
      "i 86 dloss 0.533446729183 gen_x mean 0.442687 closs 0.498195320368 cacc 0.8125\n",
      "i 110 dloss 0.497590631247 gen_x mean 0.441149 closs 0.387962549925 cacc 0.90625\n",
      "i 111 dloss 0.5015873909 gen_x mean 0.44222 closs 0.472935110331 cacc 0.8125\n",
      "i 120 dloss 0.516120493412 gen_x mean 0.430842 closs 0.583531558514 cacc 0.8125\n",
      "i 122 dloss 0.507987737656 gen_x mean 0.434365 closs 0.444340229034 cacc 0.890625\n",
      "i 141 dloss 0.508092403412 gen_x mean 0.451425 closs 0.532103300095 cacc 0.828125\n",
      "i 144 dloss 0.504518270493 gen_x mean 0.439106 closs 0.552284777164 cacc 0.796875\n",
      "i 151 dloss 0.488062471151 gen_x mean 0.436114 closs 0.464576125145 cacc 0.875\n",
      "i 157 dloss 0.515485048294 gen_x mean 0.441465 closs 0.51727437973 cacc 0.8125\n",
      "i 168 dloss 0.542134463787 gen_x mean 0.440673 closs 0.477084130049 cacc 0.828125\n",
      "i 179 dloss 0.496751844883 gen_x mean 0.436351 closs 0.353488862514 cacc 0.90625\n",
      "i 183 dloss 0.492937892675 gen_x mean 0.427539 closs 0.312751084566 cacc 0.90625\n",
      "i 184 dloss 0.477760314941 gen_x mean 0.436273 closs 0.408121645451 cacc 0.90625\n",
      "i 189 dloss 0.503153920174 gen_x mean 0.43876 closs 0.676906168461 cacc 0.765625\n",
      "i 190 dloss 0.539865672588 gen_x mean 0.433776 closs 0.580387234688 cacc 0.796875\n",
      "i 193 dloss 0.489255994558 gen_x mean 0.428508 closs 0.524660587311 cacc 0.8125\n",
      "i 207 dloss 0.529312491417 gen_x mean 0.431695 closs 0.309483766556 cacc 0.90625\n",
      "i 208 dloss 0.505960822105 gen_x mean 0.432667 closs 0.558634161949 cacc 0.8125\n",
      "i 210 dloss 0.460332512856 gen_x mean 0.428021 closs 0.230959251523 cacc 0.953125\n",
      "i 223 dloss 0.493312180042 gen_x mean 0.426016 closs 0.520780324936 cacc 0.859375\n",
      "i 224 dloss 0.498645275831 gen_x mean 0.427836 closs 0.754207909107 cacc 0.75\n",
      "i 242 dloss 0.478394687176 gen_x mean 0.427936 closs 0.540878534317 cacc 0.8125\n",
      "i 255 dloss 0.535573244095 gen_x mean 0.449623 closs 0.304281145334 cacc 0.921875\n",
      "i 262 dloss 0.523317456245 gen_x mean 0.43867 closs 0.496755808592 cacc 0.875\n",
      "i 282 dloss 0.483628720045 gen_x mean 0.444406 closs 0.683842360973 cacc 0.8125\n",
      "i 322 dloss 0.50014179945 gen_x mean 0.443836 closs 0.393208742142 cacc 0.875\n",
      "i 323 dloss 0.510971188545 gen_x mean 0.434547 closs 0.566530048847 cacc 0.8125\n",
      "i 350 dloss 0.546514928341 gen_x mean 0.437346 closs 0.475168436766 cacc 0.828125\n",
      "i 364 dloss 0.549186766148 gen_x mean 0.416678 closs 0.756410241127 cacc 0.84375\n",
      "i 369 dloss 0.509871721268 gen_x mean 0.431108 closs 0.608797311783 cacc 0.765625\n",
      "i 374 dloss 0.49031072855 gen_x mean 0.443411 closs 0.484146744013 cacc 0.84375\n",
      "i 376 dloss 0.516804933548 gen_x mean 0.444408 closs 0.902860581875 cacc 0.75\n",
      "i 386 dloss 0.506036162376 gen_x mean 0.436949 closs 0.579315423965 cacc 0.828125\n",
      "i 415 dloss 0.516918063164 gen_x mean 0.429149 closs 0.362396150827 cacc 0.875\n",
      "i 425 dloss 0.512586474419 gen_x mean 0.44214 closs 0.523780167103 cacc 0.84375\n",
      "i 435 dloss 0.506233692169 gen_x mean 0.441105 closs 0.573561906815 cacc 0.84375\n",
      "i 438 dloss 0.52145165205 gen_x mean 0.442659 closs 0.589337527752 cacc 0.78125\n",
      "i 448 dloss 0.486037939787 gen_x mean 0.421543 closs 0.425719171762 cacc 0.890625\n",
      "i 455 dloss 0.502186655998 gen_x mean 0.43651 closs 0.361650466919 cacc 0.890625\n",
      "i 462 dloss 0.511136710644 gen_x mean 0.435516 closs 0.443991094828 cacc 0.859375\n",
      "i 465 dloss 0.510174274445 gen_x mean 0.433346 closs 0.356241494417 cacc 0.890625\n",
      "i 487 dloss 0.498210608959 gen_x mean 0.454466 closs 0.419072210789 cacc 0.859375\n",
      "i 493 dloss 0.526047706604 gen_x mean 0.44247 closs 0.513688027859 cacc 0.84375\n",
      "i 495 dloss 0.496847510338 gen_x mean 0.441074 closs 0.375821679831 cacc 0.875\n",
      "i 497 dloss 0.498375505209 gen_x mean 0.432849 closs 0.307212710381 cacc 0.953125\n",
      "i 501 dloss 0.492419093847 gen_x mean 0.439317 closs 0.468414604664 cacc 0.875\n",
      "i 512 dloss 0.513453125954 gen_x mean 0.451697 closs 0.545373916626 cacc 0.78125\n",
      "i 534 dloss 0.510311126709 gen_x mean 0.429799 closs 0.446921408176 cacc 0.84375\n",
      "i 557 dloss 0.494023323059 gen_x mean 0.44332 closs 0.606917202473 cacc 0.8125\n",
      "i 558 dloss 0.473144114017 gen_x mean 0.441373 closs 0.749993860722 cacc 0.765625\n",
      "i 562 dloss 0.478217929602 gen_x mean 0.436629 closs 0.427640169859 cacc 0.875\n",
      "i 575 dloss 0.555279374123 gen_x mean 0.43702 closs 0.53490036726 cacc 0.890625\n",
      "i 589 dloss 0.526007056236 gen_x mean 0.437912 closs 0.516191124916 cacc 0.859375\n",
      "i 619 dloss 0.484147071838 gen_x mean 0.431804 closs 0.481289148331 cacc 0.859375\n",
      "i 634 dloss 0.506569385529 gen_x mean 0.428177 closs 0.481547951698 cacc 0.828125\n",
      "i 636 dloss 0.487192213535 gen_x mean 0.439516 closs 0.646839737892 cacc 0.828125\n",
      "i 648 dloss 0.510440468788 gen_x mean 0.457648 closs 0.635886669159 cacc 0.765625\n",
      "i 649 dloss 0.547016143799 gen_x mean 0.448996 closs 0.650227725506 cacc 0.71875\n",
      "i 654 dloss 0.525221526623 gen_x mean 0.445316 closs 0.779730379581 cacc 0.78125\n",
      "i 658 dloss 0.499577999115 gen_x mean 0.432039 closs 0.37051782012 cacc 0.90625\n",
      "i 669 dloss 0.501807928085 gen_x mean 0.452766 closs 0.444566577673 cacc 0.859375\n",
      "i 670 dloss 0.500118732452 gen_x mean 0.452572 closs 0.732269346714 cacc 0.84375\n",
      "i 679 dloss 0.496609181166 gen_x mean 0.432294 closs 0.622774422169 cacc 0.765625\n",
      "i 683 dloss 0.50421255827 gen_x mean 0.431023 closs 0.456578791142 cacc 0.84375\n",
      "i 703 dloss 0.610468149185 gen_x mean 0.433576 closs 0.415339231491 cacc 0.859375\n",
      "i 708 dloss 0.490983098745 gen_x mean 0.431663 closs 0.510068297386 cacc 0.859375\n",
      "i 710 dloss 0.488165080547 gen_x mean 0.436467 closs 0.562648415565 cacc 0.84375\n",
      "i 718 dloss 0.490611881018 gen_x mean 0.432726 closs 0.554703772068 cacc 0.828125\n",
      "i 730 dloss 0.516756474972 gen_x mean 0.430698 closs 0.689690053463 cacc 0.71875\n",
      "i 742 dloss 0.466248452663 gen_x mean 0.448006 closs 0.404087215662 cacc 0.859375\n",
      "i 750 dloss 0.517791450024 gen_x mean 0.448751 closs 0.43208143115 cacc 0.859375\n",
      "i 762 dloss 0.512232005596 gen_x mean 0.443238 closs 0.691207885742 cacc 0.78125\n",
      "i 776 dloss 0.510632097721 gen_x mean 0.425139 closs 0.517219007015 cacc 0.875\n",
      "i 779 dloss 0.521508336067 gen_x mean 0.442422 closs 0.550157546997 cacc 0.78125\n",
      "i 788 dloss 0.498260378838 gen_x mean 0.431498 closs 0.629906058311 cacc 0.828125\n",
      "i 790 dloss 0.49800285697 gen_x mean 0.439094 closs 0.412677139044 cacc 0.84375\n",
      "i 809 dloss 0.530561864376 gen_x mean 0.431791 closs 0.600682556629 cacc 0.828125\n",
      "i 814 dloss 0.48162779212 gen_x mean 0.441267 closs 0.500717222691 cacc 0.796875\n",
      "i 815 dloss 0.488405168056 gen_x mean 0.434229 closs 0.317358970642 cacc 0.9375\n",
      "i 818 dloss 0.523504137993 gen_x mean 0.446063 closs 0.335159987211 cacc 0.921875\n",
      "i 833 dloss 0.542458474636 gen_x mean 0.447951 closs 0.524814724922 cacc 0.828125\n",
      "i 835 dloss 0.482400029898 gen_x mean 0.447214 closs 0.636099338531 cacc 0.75\n",
      "i 849 dloss 0.499138951302 gen_x mean 0.422551 closs 0.778442621231 cacc 0.796875\n",
      "i 869 dloss 0.501726686954 gen_x mean 0.429582 closs 0.686951220036 cacc 0.796875\n",
      "i 878 dloss 0.502684593201 gen_x mean 0.425131 closs 0.392373472452 cacc 0.875\n",
      "i 897 dloss 0.513779044151 gen_x mean 0.443857 closs 0.551825881004 cacc 0.859375\n",
      "i 904 dloss 0.469721257687 gen_x mean 0.437995 closs 0.596498191357 cacc 0.84375\n",
      "i 906 dloss 0.502919435501 gen_x mean 0.426006 closs 0.712421953678 cacc 0.765625\n",
      "i 915 dloss 0.532172858715 gen_x mean 0.437162 closs 0.495521873236 cacc 0.875\n",
      "i 920 dloss 0.483424395323 gen_x mean 0.440112 closs 0.355083554983 cacc 0.84375\n",
      "i 924 dloss 0.52277058363 gen_x mean 0.445608 closs 0.294553875923 cacc 0.921875\n",
      "i 931 dloss 0.516514837742 gen_x mean 0.449476 closs 0.567071020603 cacc 0.84375\n",
      "i 950 dloss 0.485832899809 gen_x mean 0.441514 closs 0.547801733017 cacc 0.796875\n",
      "i 977 dloss 0.492919176817 gen_x mean 0.43378 closs 0.410047113895 cacc 0.875\n",
      "i 991 dloss 0.496160358191 gen_x mean 0.429143 closs 0.447342634201 cacc 0.84375\n",
      "i 996 dloss 0.484571397305 gen_x mean 0.438305 closs 0.5634958148 cacc 0.796875\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(0,500000):\n",
    "\n",
    "    z_in2 = rng.normal(size=(64,nl)).astype('float32')\n",
    "\n",
    "    if latent_sparse:\n",
    "        z_in2[:,128:] *= 0.0\n",
    "\n",
    "    r = random.randint(0,num_examples-64)\n",
    "\n",
    "    if dataset == \"mnist\":\n",
    "        x_in = trainx[r:r+64]\n",
    "        y_in = trainy[r:r+64]\n",
    "\n",
    "        x_in = x_in.reshape((64,1,28,28))\n",
    "\n",
    "        x_in = np.repeat(x_in,3,axis=(1))\n",
    "        x_in = np.lib.pad(x_in,((0,0),(0,0),(2,2),(2,2)),'constant',constant_values=(0))\n",
    "\n",
    "        x_in = x_in.reshape((64,32*32*3))\n",
    "    elif dataset == \"anime\":\n",
    "        x_in = normalize(animeData.getBatch()).reshape((64,32*32*3))\n",
    "\n",
    "    elif dataset == \"svhn\":\n",
    "\n",
    "        #just do a quick update using the whole thing.  \n",
    "\n",
    "        ind = random.randint(0,574168-64)\n",
    "        svhn_batch = svhnData.getBatch(mb_size=64,index=ind,segment=\"train\")\n",
    "        x_in2 = normalize(svhn_batch['x']).reshape((64,32*32*3))\n",
    "        train_disc_gen_faster(x_in2,z_in2)\n",
    "\n",
    "        if random.uniform(0,1) < 0.1:\n",
    "            ind = random.randint(0,num_labeled_examples_use)\n",
    "            svhn_batch = svhnData.getBatch(mb_size=64,index=ind,segment=\"hard_train\")\n",
    "            x_in2 = normalize(svhn_batch['x']).reshape((64,32*32*3))\n",
    "            y_in = svhn_batch['y']\n",
    "\n",
    "            dloss,gen_x,z_out_p,closs,cacc = train_disc_gen_classifier(x_in2,z_in2,y_in)\n",
    "\n",
    "            print \"i\", iteration, \"dloss\", dloss, \"gen_x mean\", gen_x.mean(), \"closs\", closs, \"cacc\", cacc\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "\n",
    "        if dataset == \"svhn\":\n",
    "            acclst = []\n",
    "            for ind in range(0,26032-128,64):\n",
    "            #ind = random.randint(0,26032-64)\n",
    "                testbatch = svhnData.getBatch(index=ind,mb_size=64,segment=\"test\")\n",
    "                closs,cacc = test_classifier(normalize(testbatch['x']).reshape((64,32*32*3)),testbatch['y'])\n",
    "                acclst.append(cacc)\n",
    "\n",
    "            print \"all test data\"\n",
    "            print \"test accuracy\", sum(acclst)*1.0/len(acclst)\n",
    "\n",
    "        #######plot_images(gen_x, \"plots/\" + slurm_name + \"_gen.png\")\n",
    "        plot_images(func_z_to_x(func_x_to_z(x_in2)).reshape((64,32*32*3)), \"plots/\" + slurm_name + \"_rec.png\")\n",
    "\n",
    "\n",
    "\n",
    "        dhlst = []\n",
    "        dzhlst = []\n",
    "        ylst = []\n",
    "        t0 = time.time()\n",
    "\n",
    "        dhlst_test = []\n",
    "        dzhlst_test = []\n",
    "        ylst_test = []\n",
    "\n",
    "        print \"all train set\"\n",
    "#         print \"only using eoz features\"\n",
    "\n",
    "        for ind in range(0,1000,64):\n",
    "\n",
    "            svhn_batch = svhnData.getBatch(mb_size=64,index=ind,segment=\"train\")\n",
    "            x = normalize(svhn_batch['x']).reshape((64,3*32*32))\n",
    "            y = svhn_batch['y']\n",
    "\n",
    "            zstuff = get_z(x)\n",
    "            dzstuff = get_dz(x)\n",
    "\n",
    "            #print zstuff[0].shape, zstuff[1].shape, zstuff[2].shape\n",
    "            dhlst.append(zstuff)\n",
    "            dzhlst.append(np.concatenate(dzstuff,axis=1))\n",
    "            \n",
    "            ylst.append(y)\n",
    "\n",
    "        for ind in range(0,20000,64):\n",
    "            svhn_batch = svhnData.getBatch(mb_size=64,index=ind,segment=\"test\")\n",
    "            x = normalize(svhn_batch['x']).reshape((64,3*32*32))\n",
    "            y = svhn_batch['y']\n",
    "            zstuff = get_z(x)\n",
    "            dzstuff = get_dz(x)\n",
    "            \n",
    "            dhlst_test.append(zstuff)\n",
    "            dzhlst_test.append(np.concatenate(dzstuff,axis=1))\n",
    "            \n",
    "            ylst_test.append(y)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        ##### DDDDDDZZZZZZZ\n",
    "        X_train = np.vstack(dzhlst)\n",
    "        Y_train = np.vstack(ylst).flatten()\n",
    "\n",
    "        X_test = np.vstack(dzhlst_test)\n",
    "        Y_test = np.vstack(ylst_test).flatten()\n",
    "\n",
    "        \n",
    "        C = 1.0\n",
    "        print \"DDDDDDZZZZZZZ starting training!\", \"using svm linear\", \"C\", C, \"squared hinge\"\n",
    "\n",
    "        model = svm.LinearSVC(C=C,loss='squared_hinge')\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        print \"predicting held out\"\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        testacc = np.mean(np.equal(Y_test, y_pred))\n",
    "        #print \"held out accuracy\", testacc\n",
    "\n",
    "        y_pred = model.predict(X_train)\n",
    "\n",
    "        trainacc = np.mean(np.equal(Y_train, y_pred))\n",
    "        #print \"training accuracy\", trainacc\n",
    "\n",
    "        #print time.time() - t0, \"total time to run\"\n",
    "        \n",
    "        print \"###d#\",iteration, \"testacc\", testacc, \"trainacc\", trainacc\n",
    "        zstats.append([testacc, trainacc])\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##### ZZZZZZZ\n",
    "        \n",
    "        X_train = np.vstack(dhlst)\n",
    "        Y_train = np.vstack(ylst).flatten()\n",
    "\n",
    "        X_test = np.vstack(dhlst_test)\n",
    "        Y_test = np.vstack(ylst_test).flatten()\n",
    "\n",
    "\n",
    "        C = 1.0\n",
    "        print \"ZZZZZZ starting training!\", \"using svm linear\", \"C\", C, \"squared hinge\"\n",
    "\n",
    "        model = svm.LinearSVC(C=C,loss='squared_hinge')\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        print \"predicting held out\"\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        testacc = np.mean(np.equal(Y_test, y_pred))\n",
    "        #print \"held out accuracy\", testacc\n",
    "\n",
    "        y_pred = model.predict(X_train)\n",
    "\n",
    "        trainacc = np.mean(np.equal(Y_train, y_pred))\n",
    "        #print \"training accuracy\", trainacc\n",
    "\n",
    "        \n",
    "        print \"###dz#\",iteration, \"testacc\", testacc, \"trainacc\", trainacc\n",
    "        dzstats.append([testacc, trainacc])\n",
    "        \n",
    "        statsname = \"stats.pkl\"\n",
    "        print \"writing\", statsname, time.time() - t0, \"total time to run\"\n",
    "\n",
    "        pickle.dump([zstats,dzstats], open(statsname, \"w\", 0))\n",
    "        \n",
    "        paramsfile = str(iteration) + \"baseparamsfile\"\n",
    "        print \"writing\"\n",
    "        save_network(gparams,dparams, cparams,paramsfile)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_network(gparams,dparams, cparams,\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fadb2f42f10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fadb31fef50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.asarray(dzstats)[:,1], label=\"Gen Train\", linestyle='-', c=\"blue\")\n",
    "plt.plot(np.asarray(dzstats)[:,0], label=\"Gen Test\", linestyle='--', c=\"blue\")\n",
    "\n",
    "plt.plot(np.asarray(zstats)[:,1], label=\"Disc Train\", linestyle='-', c=\"red\")\n",
    "plt.plot(np.asarray(zstats)[:,0], label=\"Disc Test\", linestyle='--', c=\"red\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from fuel.converters.base import fill_hdf5_file\n",
    "from fuel.datasets import SVHN\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ndim' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-68abaa0590ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_epoch_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'ndim' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "normalize(train_stream.get_epoch_iterator().next()[0]).flatten(ndim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_stream.get_epoch_iterator().next()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.00146951,  0.02156572,  0.13495095, ...,  0.03146654,\n",
       "         0.00323646, -0.00081727], dtype=float32), array([[1],\n",
       "        [9],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [5],\n",
       "        [9],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [8],\n",
       "        [7],\n",
       "        [4],\n",
       "        [4],\n",
       "        [1],\n",
       "        [2],\n",
       "        [8],\n",
       "        [1],\n",
       "        [6],\n",
       "        [2],\n",
       "        [3],\n",
       "        [6],\n",
       "        [3],\n",
       "        [4],\n",
       "        [2],\n",
       "        [5],\n",
       "        [8],\n",
       "        [1],\n",
       "        [6],\n",
       "        [2],\n",
       "        [3],\n",
       "        [7],\n",
       "        [9],\n",
       "        [5],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [6],\n",
       "        [2],\n",
       "        [2],\n",
       "        [5],\n",
       "        [1],\n",
       "        [5],\n",
       "        [4],\n",
       "        [7],\n",
       "        [8],\n",
       "        [9],\n",
       "        [6],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [5],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1]], dtype=int32)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess(*train_stream.get_epoch_iterator().next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turned on injected noise in x->z connection\n",
      "special thing in D\n",
      "train data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "GpuReshape: trying to reshape an array of total size 307200 into an array of total size 196608.\nApply node that caused the error: GpuReshape{4}(GpuFromHost<None>.0, TensorConstant{[64  3 32 32]})\nToposort index: 91\nInputs types: [GpuArrayType<None>(float32, (False, False)), TensorType(int64, vector)]\nInputs shapes: [(100, 3072), (4,)]\nInputs strides: [(12288, 4), (8,)]\nInputs values: ['not shown', array([64,  3, 32, 32])]\nOutputs clients: [[HostFromGpu(gpuarray)(GpuReshape{4}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-3c5afbcab254>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     list(zip(*[preprocess(*batch) for batch in\n\u001b[1;32m---> 34\u001b[1;33m                train_stream.get_epoch_iterator()])))\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"test data\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-107-3c5afbcab254>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mprepreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/cohenjos/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    896\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/cohenjos/.local/lib/python2.7/site-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/cohenjos/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: GpuReshape: trying to reshape an array of total size 307200 into an array of total size 196608.\nApply node that caused the error: GpuReshape{4}(GpuFromHost<None>.0, TensorConstant{[64  3 32 32]})\nToposort index: 91\nInputs types: [GpuArrayType<None>(float32, (False, False)), TensorType(int64, vector)]\nInputs shapes: [(100, 3072), (4,)]\nInputs strides: [(12288, 4), (8,)]\nInputs values: ['not shown', array([64,  3, 32, 32])]\nOutputs clients: [[HostFromGpu(gpuarray)(GpuReshape{4}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "\n",
    "input_x = T.matrix('x')\n",
    "input_y = T.imatrix('targets')\n",
    "\n",
    "\n",
    "z_inf,z_feat = x_to_z(gparams, inverse_sigmoid(input_x))\n",
    "\n",
    "d_val, d_feat = discriminator(dparams, input_x, z_inf)\n",
    "\n",
    "#get_dz = theano.function([input_x], outputs = [d_feat[0],d_feat[1]])\n",
    "\n",
    "#preprocess = theano.function([x, y], [output.flatten(ndim=2), y])\n",
    "\n",
    "\n",
    "prepreprocess = theano.function([input_x, input_y], outputs = [T.concatenate([d_feat[0],d_feat[1]], axis=1).flatten(),input_y])\n",
    "\n",
    "\n",
    "def preprocess(x,y):\n",
    "    return prepreprocess(normalize(x).reshape((-1,32*32*3)),y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#h5file = h5py.File(\"ALI-semisuper-disc\", mode='w')\n",
    "\n",
    "print \"train data\"\n",
    "train_set = SVHN(2, which_sets=('train',), sources=('features', 'targets'))\n",
    "train_stream = DataStream.default_stream(\n",
    "    train_set,\n",
    "    iteration_scheme=SequentialScheme(train_set.num_examples, 100))\n",
    "train_features, train_targets = map(\n",
    "    np.vstack,\n",
    "    list(zip(*[preprocess(*batch) for batch in\n",
    "               train_stream.get_epoch_iterator()])))\n",
    "\n",
    "print \"test data\"\n",
    "test_set = SVHN(2, which_sets=('test',), sources=('features', 'targets'))\n",
    "test_stream = DataStream.default_stream(\n",
    "    test_set,\n",
    "    iteration_scheme=SequentialScheme(test_set.num_examples, 100))\n",
    "test_features, test_targets = map(\n",
    "    numpy.vstack,\n",
    "    list(zip(*[preprocess(*batch) for batch in\n",
    "               test_stream.get_epoch_iterator()])))\n",
    "\n",
    "data = (('train', 'features', train_features),\n",
    "        ('test', 'features', test_features),\n",
    "        ('train', 'targets', train_targets),\n",
    "        ('test', 'targets', test_targets))\n",
    "fill_hdf5_file(h5file, data)\n",
    "for i, label in enumerate(('batch', 'feature')):\n",
    "    h5file['features'].dims[i].label = label\n",
    "for i, label in enumerate(('batch', 'index')):\n",
    "    h5file['targets'].dims[i].label = label\n",
    "\n",
    "h5file.flush()\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
