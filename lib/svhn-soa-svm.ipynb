{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5105 on context None\n",
      "Mapped name None to device cuda0: Quadro K6000 (0000:04:00.0)\n",
      "/Tmp/lisa/os_v5/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python \n",
    "\n",
    "'''\n",
    "-Initially make z_to_x and x_to_z fairly shallow networks.  Inject noise?  \n",
    "\n",
    "-Use the fflayer class?  \n",
    "\n",
    "'''\n",
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "sys.path.append(\"/u/lambalex/DeepLearning/undirected_matching\")\n",
    "sys.path.append(\"/u/lambalex/DeepLearning/undirected_matching/lib\")\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from nn_layers import fflayer, param_init_fflayer, param_init_convlayer, convlayer\n",
    "from utils import init_tparams, join2, srng, dropout, inverse_sigmoid, join3, merge_images\n",
    "from loss import accuracy, crossent, lsgan_loss, wgan_loss, improvement_loss\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "import random\n",
    "from viz import plot_images\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "import os\n",
    "slurm_name = os.environ[\"SLURM_JOB_ID\"]\n",
    "\n",
    "class ConsiderConstant(theano.compile.ViewOp):\n",
    "    def grad(self, args, g_outs):\n",
    "        return [T.zeros_like(g_out) for g_out in g_outs]\n",
    "\n",
    "consider_constant = ConsiderConstant()\n",
    "\n",
    "\n",
    "\n",
    "def init_gparams(p):\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='z_x_1',nin=nl*2,nout=512*4*4,ortho=False,batch_norm=True)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_2',nin=512,nout=256,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_3',nin=256*1,nout=128,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_4',nin=128*1,nout=3,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_1',nin=3,nout=32,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_2',nin=32,nout=64,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_3',nin=64,nout=128,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_4',nin=128,nout=256,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_5',nin=256,nout=512,kernel_len=5,batch_norm=True)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_fc1',nin=512*4*4,nout=1024,ortho=False,batch_norm=True)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_fc2',nin=1024,nout=1024,ortho=False,batch_norm=True)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_mu',nin=1024,nout=nl,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_sigma',nin=1024,nout=nl,ortho=False,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "def init_cparams(p):\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='c_1',nin=nl+512*4*4,nout=512,ortho=False,batch_norm=True)\n",
    "    print \"mlp on top, 512 dim\"\n",
    "    p = param_init_fflayer(options={},params=p,prefix='c_2',nin=512,nout=512,ortho=False,batch_norm=True)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='c_3',nin=512,nout=10,ortho=False,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "def init_dparams(p):\n",
    "\n",
    "    print \"NOT trying batch norm in the discriminator part that sees x!\"\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_1',nin=3,nout=32,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_2',nin=32,nout=64,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_3',nin=64,nout=128,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_4',nin=128,nout=256,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_5',nin=256,nout=512,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_1',nin=nl+512*4*4,nout=nfd,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_2',nin=nfd,nout=nfd,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_3',nin=nfd,nout=nfd,ortho=False,batch_norm=False)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_1',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_2',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_3',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_4',nin=128,nout=1,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_5',nin=256,nout=1,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_6',nin=512,nout=1,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "\n",
    "def z_to_x(p,z):\n",
    "\n",
    "    print \"no extra noise input\"\n",
    "    z_inp = join2(z, 0.0*srng.normal(size=z.shape))\n",
    "\n",
    "    d0 = fflayer(tparams=p,state_below=z_inp,options={},prefix='z_x_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    d0 = d0.reshape((64,512,4,4))\n",
    "\n",
    "    d1 = convlayer(tparams=p,state_below=d0,options={},prefix='z_x_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=-2)\n",
    "\n",
    "    d2 = convlayer(tparams=p,state_below=d1,options={},prefix='z_x_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=-2)\n",
    "\n",
    "    d3 = convlayer(tparams=p,state_below=d2,options={},prefix='z_x_4',activ='lambda x: x',stride=-2)\n",
    "\n",
    "    x_new = d3.flatten(2)\n",
    "\n",
    "    return x_new\n",
    "\n",
    "def x_to_z(p,x):\n",
    "\n",
    "    e1 = convlayer(tparams=p,state_below=x.reshape((64,3,32,32)),options={},prefix='x_z_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=1)\n",
    "\n",
    "    e2 = convlayer(tparams=p,state_below=e1,options={},prefix='x_z_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    e3 = convlayer(tparams=p,state_below=e2,options={},prefix='x_z_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=1)\n",
    "\n",
    "    e4 = convlayer(tparams=p,state_below=e3,options={},prefix='x_z_4',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    e5 = convlayer(tparams=p,state_below=e4,options={},prefix='x_z_5',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    eo = e5\n",
    "    eo = eo.flatten(2)\n",
    "\n",
    "    encoder_features = eo\n",
    "\n",
    "    h1 = fflayer(tparams=p,state_below=eo,options={},prefix='x_z_fc1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "    h2 = fflayer(tparams=p,state_below=h1,options={},prefix='x_z_fc2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    sigma = fflayer(tparams=p,state_below=h2,options={},prefix='x_z_mu',activ='lambda x: x')\n",
    "    mu = fflayer(tparams=p,state_below=h2,options={},prefix='x_z_sigma',activ='lambda x: x')\n",
    "\n",
    "    eps = srng.normal(size=sigma.shape)\n",
    "\n",
    "    z_new = eps*T.nnet.sigmoid(sigma) + mu\n",
    "    print \"turned on injected noise in x->z connection\"\n",
    "\n",
    "    z_new = (z_new - T.mean(z_new, axis=0, keepdims=True)) / (0.001 + T.std(z_new, axis=0, keepdims=True))\n",
    "\n",
    "    return z_new,encoder_features\n",
    "\n",
    "def classifier(p,z,true_y):\n",
    "\n",
    "    print \"turning off gradients from classifier\"\n",
    "    z = consider_constant(z)\n",
    "\n",
    "    h1 = fflayer(tparams=p,state_below=z,options={},prefix='c_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    h2 = fflayer(tparams=p,state_below=h1,options={},prefix='c_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    y_est = fflayer(tparams=p,state_below=h2,options={},prefix='c_3',activ='lambda x: x')\n",
    "\n",
    "    y_est = T.nnet.softmax(y_est)\n",
    "\n",
    "    acc = accuracy(y_est,true_y)\n",
    "    loss = crossent(y_est,true_y)\n",
    "\n",
    "    return loss,acc\n",
    "\n",
    "def discriminator(p,x,z):\n",
    "\n",
    "    dc_1 = convlayer(tparams=p,state_below=x.reshape((64,3,32,32)),options={},prefix='DC_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=1)\n",
    "\n",
    "    dc_2 = convlayer(tparams=p,state_below=dc_1,options={},prefix='DC_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    dc_3 = convlayer(tparams=p,state_below=dc_2,options={},prefix='DC_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=1)\n",
    "\n",
    "    dc_4 = convlayer(tparams=p,state_below=dc_3,options={},prefix='DC_4',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    dc_5 = convlayer(tparams=p,state_below=dc_4,options={},prefix='DC_5',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    inp = join2(z,dc_5.flatten(2))\n",
    "\n",
    "    h1 = fflayer(tparams=p,state_below=inp,options={},prefix='D_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',mean_ln=False)\n",
    "\n",
    "    h2 = fflayer(tparams=p,state_below=h1,options={},prefix='D_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)', mean_ln=False)\n",
    "\n",
    "    h3 = fflayer(tparams=p,state_below=h2,options={},prefix='D_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)', mean_ln=False)\n",
    "\n",
    "    D1 = fflayer(tparams=p,state_below=h1,options={},prefix='D_o_1',activ='lambda x: x')\n",
    "    D2 = fflayer(tparams=p,state_below=h2,options={},prefix='D_o_2',activ='lambda x: x')\n",
    "    D3 = fflayer(tparams=p,state_below=h3,options={},prefix='D_o_3',activ='lambda x: x')\n",
    "\n",
    "    D4 = convlayer(tparams=p,state_below=dc_3,options={},prefix='D_o_4',activ='lambda x: x',stride=2)\n",
    "    D5 = convlayer(tparams=p,state_below=dc_4,options={},prefix='D_o_5',activ='lambda x: x',stride=2)\n",
    "    D6 = convlayer(tparams=p,state_below=dc_5,options={},prefix='D_o_6',activ='lambda x: x',stride=2)\n",
    "\n",
    "    print \"special thing in D\"\n",
    "    return [D1,D2,D3,D4,D5,D6], [h3,dc_5.flatten(2)]\n",
    "\n",
    "def p_chain(p, z, num_iterations):\n",
    "    zlst = [z]\n",
    "    xlst = []\n",
    "\n",
    "    if num_iterations == 1:\n",
    "        \n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        #new_z = x_to_z(p, xlst[-1])\n",
    "        #zlst.append(new_z)\n",
    "\n",
    "    elif num_iterations == 3:  \n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        new_z,_ = x_to_z(p, consider_constant(xlst[-1]))\n",
    "        zlst.append(new_z)\n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        new_z,_ = x_to_z(p, consider_constant(xlst[-1]))\n",
    "        zlst.append(new_z)\n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "\n",
    "    else:\n",
    "\n",
    "        for inds in range(0,num_iterations):\n",
    "            new_x = z_to_x(p, zlst[-1])\n",
    "            xlst.append(new_x)\n",
    "            new_z,_ = x_to_z(p, xlst[-1])\n",
    "            zlst.append(new_z)\n",
    "\n",
    "\n",
    "    for j in range(len(xlst)):\n",
    "        xlst[j] = T.nnet.sigmoid(xlst[j])\n",
    "\n",
    "    return xlst, zlst\n",
    "\n",
    "def onestep_z_to_x(p,z):\n",
    "    x = T.nnet.sigmoid(z_to_x(p, z))\n",
    "    return x\n",
    "\n",
    "def onestep_x_to_z(p,x):\n",
    "    new_z,_ = x_to_z(p, inverse_sigmoid(x))\n",
    "    return new_z\n",
    "\n",
    "def q_chain(p,x,num_iterations):\n",
    "\n",
    "    xlst = [x]\n",
    "    zlst = []\n",
    "    new_z,encoder_features = x_to_z(p, inverse_sigmoid(xlst[-1]))\n",
    "    zlst.append(new_z)\n",
    "\n",
    "    return xlst, zlst,encoder_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num latent 128\n",
      "num steps 3\n",
      "latent sparse False\n",
      "improvement loss weight 0.0\n",
      "num labeled examples used 50000\n",
      "dataset svhn\n",
      "num train examples 604388\n",
      "num test examples 26032\n",
      "NOT trying batch norm in the discriminator part that sees x!\n",
      "mlp on top, 512 dim\n",
      "no extra noise input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Quadro K6000 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "special thing in D\n",
      "single disc\n",
      "special thing in D\n",
      "turning off gradients from classifier\n",
      "not using improvement objective\n",
      "no extra noise input\n",
      "turned on injected noise in x->z connection\n"
     ]
    }
   ],
   "source": [
    "nl = 128\n",
    "print \"num latent\", nl\n",
    "#128 works for nl\n",
    "nfg = 512\n",
    "nfd = 512\n",
    "\n",
    "\n",
    "#3\n",
    "num_steps = 3\n",
    "print \"num steps\", num_steps\n",
    "\n",
    "latent_sparse = False\n",
    "print \"latent sparse\", latent_sparse\n",
    "\n",
    "improvement_loss_weight = 0.0\n",
    "print \"improvement loss weight\", improvement_loss_weight\n",
    "\n",
    "num_labeled_examples_use = 50000\n",
    "print \"num labeled examples used\",num_labeled_examples_use\n",
    "\n",
    "#dataset = \"mnist\"\n",
    "#dataset = \"anime\"\n",
    "dataset = \"svhn\"\n",
    "print \"dataset\", dataset\n",
    "\n",
    "if dataset == \"mnist\":\n",
    "    mn = gzip.open(\"/u/lambalex/data/mnist/mnist.pkl.gz\")\n",
    "\n",
    "    train, valid, test = pickle.load(mn)\n",
    "\n",
    "    trainx,trainy = train\n",
    "\n",
    "\n",
    "    #newtx = trainx[(trainy<2) | (trainy>8)]\n",
    "    #newty = trainy[(trainy<2) | (trainy>8)]\n",
    "    #trainx = newtx\n",
    "    #trainy = newty\n",
    "\n",
    "    validx,validy = valid\n",
    "    testx, testy = test\n",
    "\n",
    "    num_examples = trainx.shape[0]\n",
    "\n",
    "    m = 784\n",
    "\n",
    "elif dataset == \"anime\":\n",
    "    from load_file import FileData, normalize, denormalize\n",
    "\n",
    "    loc = \"/u/lambalex/DeepLearning/animefaces/datafaces/danbooru-faces/\"\n",
    "\n",
    "    animeData = FileData(loc, 32, 64)\n",
    "\n",
    "    m = 32*32*3\n",
    "\n",
    "elif dataset == \"svhn\":\n",
    "\n",
    "    from load_svhn import SvhnData\n",
    "    from load_file import normalize, denormalize\n",
    "\n",
    "    svhnData = SvhnData()\n",
    "\n",
    "    num_examples = 50000\n",
    "\n",
    "\n",
    "\n",
    "gparams = init_gparams({})\n",
    "dparams = init_dparams({})\n",
    "cparams = init_cparams({})\n",
    "\n",
    "z_in = T.matrix('z_in')\n",
    "x_in = T.matrix()\n",
    "true_y = T.ivector('true_y')\n",
    "\n",
    "p_lst_x,p_lst_z = p_chain(gparams, z_in, num_steps)\n",
    "\n",
    "q_lst_x,q_lst_z,encoder_features = q_chain(gparams, x_in, num_steps)\n",
    "\n",
    "p_lst_x_long,p_lst_z_long = p_chain(gparams, z_in, 19)\n",
    "\n",
    "z_inf = q_lst_z[-1]\n",
    "\n",
    "D_p_lst_1,_ = discriminator(dparams, p_lst_x[-1], p_lst_z[-1])\n",
    "\n",
    "if False:\n",
    "    D_p_lst_2,_ = discriminator(dparams, p_lst_x[-2], p_lst_z[-2])\n",
    "    D_p_lst = D_p_lst_1 + D_p_lst_2\n",
    "    print \"double disc\"\n",
    "else:\n",
    "    D_p_lst = D_p_lst_1\n",
    "    print \"single disc\"\n",
    "\n",
    "D_q_lst,D_feat_q = discriminator(dparams, q_lst_x[-1], q_lst_z[-1])\n",
    "\n",
    "closs,cacc = classifier(cparams,join2(z_inf,encoder_features),true_y)\n",
    "\n",
    "dloss, gloss = lsgan_loss(D_q_lst, D_p_lst)\n",
    "\n",
    "print \"not using improvement objective\"\n",
    "#improvement_objective = improvement_loss_weight * improvement_loss(D_p_lst_1, D_p_lst_2)\n",
    "#gloss += improvement_objective\n",
    "\n",
    "dupdates = lasagne.updates.rmsprop(dloss, dparams.values(),0.0001)\n",
    "gloss_grads = T.grad(gloss, gparams.values(), disconnected_inputs='ignore')\n",
    "gupdates = lasagne.updates.rmsprop(gloss_grads, gparams.values(),0.0001)\n",
    "\n",
    "gcupdates = lasagne.updates.rmsprop(gloss + closs, gparams.values() + cparams.values(),0.0001)\n",
    "dcupdates = lasagne.updates.rmsprop(dloss + closs, dparams.values() + cparams.values(),0.0001)\n",
    "\n",
    "dgupdates = dupdates.copy()\n",
    "dgupdates.update(gupdates)\n",
    "\n",
    "dgcupdates = dcupdates.copy()\n",
    "dgcupdates.update(gcupdates)\n",
    "\n",
    "train_disc_gen_classifier = theano.function(inputs = [x_in, z_in,true_y], outputs=[dloss,p_lst_x[-1],p_lst_z[-1],closs,cacc], updates=dgcupdates,on_unused_input='ignore')\n",
    "\n",
    "train_disc_gen = theano.function(inputs = [x_in, z_in], outputs=[dloss,p_lst_x[-1],p_lst_z[-1]], updates=dgupdates,on_unused_input='ignore')\n",
    "\n",
    "train_disc_gen_faster = theano.function(inputs = [x_in, z_in], outputs=dloss, updates=dgupdates,on_unused_input='ignore')\n",
    "\n",
    "test_classifier = theano.function(inputs = [x_in,true_y], outputs=[closs,cacc],on_unused_input='ignore')\n",
    "\n",
    "get_zinf = theano.function([x_in], outputs=z_inf)\n",
    "#get_dfeat = theano.function([x_in], outputs=D_feat_q)\n",
    "\n",
    "#get_pchain = theano.function([z_in], outputs = p_lst_x_long)\n",
    "\n",
    "x_in = T.matrix()\n",
    "\n",
    "func_z_to_x = theano.function([z_in], outputs = onestep_z_to_x(gparams, z_in))\n",
    "func_x_to_z = theano.function([x_in], outputs = onestep_x_to_z(gparams, x_in))\n",
    "\n",
    "z_out_p = rng.normal(size=(64,nl)).astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turned on injected noise in x->z connection\n",
      "special thing in D\n"
     ]
    }
   ],
   "source": [
    "input_x = T.matrix('x')\n",
    "\n",
    "z_inf,z_feat = x_to_z(gparams, inverse_sigmoid(input_x))\n",
    "\n",
    "d_val, d_feat = discriminator(dparams, input_x, z_inf)\n",
    "\n",
    "#z_feat, d_feat[1]\n",
    "get_dz = theano.function([input_x], outputs = [d_feat[0],d_feat[1]])\n",
    "\n",
    "get_z = func_x_to_z\n",
    "\n",
    "\n",
    "#rec = theano.function([input_x], outputs = [T.nnet.sigmoid(z_to_x(gparams,x_to_z(gparams,inverse_sigmoid(input_x))[0]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_dz(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zstats = []\n",
    "dzstats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all test data\n",
      "test accuracy 0.153086419753\n",
      "all train set\n",
      "DDDDDDZZZZZZZ starting training! using svm linear C 1.0 squared hinge\n",
      "predicting held out\n",
      "###d# 0 testacc 0.131389776358 trainacc 0.7021484375\n",
      "ZZZZZZ starting training! using svm linear C 1.0 squared hinge\n",
      "predicting held out\n",
      "###dz# 0 testacc 0.120507188498 trainacc 0.4267578125\n",
      "wrote stats.pkl 60.41395998 total time to run\n",
      "i 3 dloss 0.959126472473 gen_x mean 0.507195 closs 2.29938411713 cacc 0.1875\n",
      "i 6 dloss 0.937801063061 gen_x mean 0.508446 closs 2.28396987915 cacc 0.140625\n",
      "i 21 dloss 0.730803370476 gen_x mean 0.606592 closs 2.24814033508 cacc 0.15625\n",
      "i 35 dloss 0.663992762566 gen_x mean 0.706525 closs 2.29466557503 cacc 0.1875\n",
      "i 41 dloss 0.662517488003 gen_x mean 0.738792 closs 2.27445459366 cacc 0.171875\n",
      "i 48 dloss 0.631886005402 gen_x mean 0.763902 closs 2.24623918533 cacc 0.1875\n",
      "i 53 dloss 0.597481966019 gen_x mean 0.774883 closs 2.29751896858 cacc 0.1875\n",
      "i 56 dloss 0.554739177227 gen_x mean 0.776342 closs 2.27238917351 cacc 0.265625\n",
      "i 57 dloss 0.555992543697 gen_x mean 0.774685 closs 2.24425292015 cacc 0.21875\n",
      "i 58 dloss 0.533248901367 gen_x mean 0.771066 closs 2.26695871353 cacc 0.1875\n",
      "i 67 dloss 0.410723686218 gen_x mean 0.735859 closs 2.22055649757 cacc 0.234375\n",
      "i 70 dloss 0.39813798666 gen_x mean 0.720582 closs 2.26222658157 cacc 0.203125\n",
      "i 102 dloss 0.3429941535 gen_x mean 0.594658 closs 2.2957239151 cacc 0.140625\n",
      "i 105 dloss 0.359651088715 gen_x mean 0.588334 closs 2.26989293098 cacc 0.09375\n",
      "i 106 dloss 0.328902184963 gen_x mean 0.583311 closs 2.23441696167 cacc 0.203125\n",
      "i 112 dloss 0.378793001175 gen_x mean 0.566953 closs 2.26364541054 cacc 0.234375\n",
      "i 114 dloss 0.352517068386 gen_x mean 0.555883 closs 2.27005720139 cacc 0.171875\n",
      "i 115 dloss 0.441961109638 gen_x mean 0.549448 closs 2.24921917915 cacc 0.171875\n",
      "i 150 dloss 0.35761475563 gen_x mean 0.450145 closs 2.22759485245 cacc 0.234375\n",
      "i 158 dloss 0.338296234608 gen_x mean 0.438427 closs 2.25422883034 cacc 0.15625\n",
      "i 162 dloss 0.432810008526 gen_x mean 0.43092 closs 2.27697086334 cacc 0.15625\n",
      "i 172 dloss 0.500560998917 gen_x mean 0.417709 closs 2.2833096981 cacc 0.140625\n",
      "i 194 dloss 0.518560349941 gen_x mean 0.402624 closs 2.26168751717 cacc 0.125\n",
      "i 195 dloss 0.517655313015 gen_x mean 0.406034 closs 2.26905107498 cacc 0.1875\n",
      "i 196 dloss 0.528083562851 gen_x mean 0.40948 closs 2.25178980827 cacc 0.203125\n",
      "i 201 dloss 0.511608481407 gen_x mean 0.42375 closs 2.23131227493 cacc 0.25\n",
      "i 206 dloss 0.521617233753 gen_x mean 0.430169 closs 2.26227927208 cacc 0.125\n",
      "i 208 dloss 0.509044826031 gen_x mean 0.426218 closs 2.21467208862 cacc 0.234375\n",
      "i 214 dloss 0.495204150677 gen_x mean 0.433983 closs 2.22078680992 cacc 0.21875\n",
      "i 228 dloss 0.486608803272 gen_x mean 0.432018 closs 2.25374031067 cacc 0.140625\n",
      "i 244 dloss 0.485153913498 gen_x mean 0.434752 closs 2.29089736938 cacc 0.109375\n",
      "i 245 dloss 0.488497912884 gen_x mean 0.441941 closs 2.21562886238 cacc 0.171875\n",
      "i 256 dloss 0.46239233017 gen_x mean 0.433173 closs 2.17432355881 cacc 0.328125\n",
      "i 263 dloss 0.463045805693 gen_x mean 0.424932 closs 2.16361546516 cacc 0.1875\n",
      "i 272 dloss 0.463348478079 gen_x mean 0.433469 closs 2.25991153717 cacc 0.203125\n",
      "i 274 dloss 0.466657966375 gen_x mean 0.422574 closs 2.28009724617 cacc 0.171875\n",
      "i 283 dloss 0.484348535538 gen_x mean 0.433797 closs 2.28825473785 cacc 0.21875\n",
      "i 298 dloss 0.486882448196 gen_x mean 0.455205 closs 2.23139953613 cacc 0.1875\n",
      "i 321 dloss 0.431946456432 gen_x mean 0.451117 closs 2.26636695862 cacc 0.203125\n",
      "i 324 dloss 0.468319654465 gen_x mean 0.456969 closs 2.30004882812 cacc 0.140625\n",
      "i 340 dloss 0.449380218983 gen_x mean 0.451326 closs 2.24333620071 cacc 0.265625\n",
      "i 346 dloss 0.462895750999 gen_x mean 0.452085 closs 2.27982115746 cacc 0.21875\n",
      "i 348 dloss 0.442474603653 gen_x mean 0.439581 closs 2.29121446609 cacc 0.171875\n",
      "i 351 dloss 0.552032291889 gen_x mean 0.445982 closs 2.24744963646 cacc 0.1875\n",
      "i 357 dloss 0.48288449645 gen_x mean 0.447072 closs 2.31356072426 cacc 0.125\n",
      "i 366 dloss 0.485122621059 gen_x mean 0.460808 closs 2.25490617752 cacc 0.1875\n",
      "i 367 dloss 0.497277468443 gen_x mean 0.454303 closs 2.2060072422 cacc 0.265625\n",
      "i 368 dloss 0.495078504086 gen_x mean 0.461634 closs 2.24855947495 cacc 0.15625\n",
      "i 371 dloss 0.495406359434 gen_x mean 0.470605 closs 2.27694320679 cacc 0.171875\n",
      "i 392 dloss 0.540998578072 gen_x mean 0.451014 closs 2.24918675423 cacc 0.1875\n",
      "i 420 dloss 0.454702198505 gen_x mean 0.458594 closs 2.27252078056 cacc 0.203125\n",
      "i 428 dloss 0.497537314892 gen_x mean 0.440366 closs 2.28678560257 cacc 0.15625\n",
      "i 430 dloss 0.440179169178 gen_x mean 0.457143 closs 2.24599766731 cacc 0.109375\n",
      "i 466 dloss 0.462937891483 gen_x mean 0.43132 closs 2.25500893593 cacc 0.140625\n",
      "i 480 dloss 0.519412398338 gen_x mean 0.440098 closs 2.29203748703 cacc 0.171875\n",
      "i 497 dloss 0.489843428135 gen_x mean 0.458196 closs 2.24465870857 cacc 0.203125\n",
      "i 513 dloss 0.534418940544 gen_x mean 0.444985 closs 2.28466939926 cacc 0.15625\n",
      "i 524 dloss 0.493291735649 gen_x mean 0.434472 closs 2.29471445084 cacc 0.15625\n",
      "i 528 dloss 0.544224977493 gen_x mean 0.442056 closs 2.16570973396 cacc 0.1875\n",
      "i 541 dloss 0.560971617699 gen_x mean 0.47069 closs 2.21784639359 cacc 0.1875\n",
      "i 554 dloss 0.533417582512 gen_x mean 0.473217 closs 2.21339011192 cacc 0.1875\n",
      "i 559 dloss 0.495203465223 gen_x mean 0.473535 closs 2.24102497101 cacc 0.140625\n",
      "i 560 dloss 0.479960918427 gen_x mean 0.471169 closs 2.21897959709 cacc 0.171875\n",
      "i 572 dloss 0.476297110319 gen_x mean 0.469376 closs 2.22341823578 cacc 0.203125\n",
      "i 577 dloss 0.490851759911 gen_x mean 0.472357 closs 2.12140989304 cacc 0.28125\n",
      "i 578 dloss 0.483964562416 gen_x mean 0.468782 closs 2.30791425705 cacc 0.15625\n",
      "i 591 dloss 0.490113854408 gen_x mean 0.470428 closs 2.15739250183 cacc 0.265625\n",
      "i 594 dloss 0.508121728897 gen_x mean 0.468604 closs 2.2025642395 cacc 0.234375\n",
      "i 601 dloss 0.507424175739 gen_x mean 0.456351 closs 2.25798845291 cacc 0.125\n",
      "i 609 dloss 0.474655002356 gen_x mean 0.442973 closs 2.2690448761 cacc 0.1875\n",
      "i 611 dloss 0.513996303082 gen_x mean 0.446471 closs 2.12101817131 cacc 0.328125\n",
      "i 623 dloss 0.531581878662 gen_x mean 0.451289 closs 2.25336408615 cacc 0.125\n",
      "i 647 dloss 0.543059706688 gen_x mean 0.446166 closs 2.16804671288 cacc 0.203125\n",
      "i 698 dloss 0.512726902962 gen_x mean 0.423971 closs 2.23063349724 cacc 0.15625\n",
      "i 702 dloss 0.476207017899 gen_x mean 0.422257 closs 2.22966170311 cacc 0.15625\n",
      "i 710 dloss 0.50257897377 gen_x mean 0.414481 closs 2.23017024994 cacc 0.15625\n",
      "i 733 dloss 0.50621265173 gen_x mean 0.410936 closs 2.22313833237 cacc 0.203125\n",
      "i 747 dloss 0.494198024273 gen_x mean 0.413674 closs 2.31124377251 cacc 0.171875\n",
      "i 750 dloss 0.497168540955 gen_x mean 0.418032 closs 2.23699641228 cacc 0.171875\n",
      "i 755 dloss 0.487627238035 gen_x mean 0.414983 closs 2.28941130638 cacc 0.125\n",
      "i 775 dloss 0.553813874722 gen_x mean 0.42846 closs 2.25567603111 cacc 0.203125\n",
      "i 790 dloss 0.493277639151 gen_x mean 0.428596 closs 2.26787424088 cacc 0.1875\n",
      "i 821 dloss 0.506859898567 gen_x mean 0.430515 closs 2.29654717445 cacc 0.1875\n",
      "i 835 dloss 0.49959358573 gen_x mean 0.43883 closs 2.16222214699 cacc 0.1875\n",
      "i 845 dloss 0.5057310462 gen_x mean 0.424403 closs 2.15719842911 cacc 0.265625\n",
      "i 859 dloss 0.517495214939 gen_x mean 0.435544 closs 2.26067280769 cacc 0.203125\n",
      "i 868 dloss 0.498741835356 gen_x mean 0.422856 closs 2.13320565224 cacc 0.28125\n",
      "i 882 dloss 0.493400156498 gen_x mean 0.42677 closs 2.15526604652 cacc 0.203125\n",
      "i 889 dloss 0.513620376587 gen_x mean 0.441428 closs 2.20713639259 cacc 0.171875\n",
      "i 895 dloss 0.503399252892 gen_x mean 0.434462 closs 2.23182487488 cacc 0.171875\n",
      "i 897 dloss 0.487643778324 gen_x mean 0.429549 closs 2.18229269981 cacc 0.21875\n",
      "i 904 dloss 0.503049492836 gen_x mean 0.42882 closs 2.29660463333 cacc 0.171875\n",
      "i 906 dloss 0.502898454666 gen_x mean 0.433207 closs 2.22932934761 cacc 0.140625\n",
      "i 908 dloss 0.49124777317 gen_x mean 0.43136 closs 2.3146674633 cacc 0.171875\n",
      "i 921 dloss 0.4835293293 gen_x mean 0.422818 closs 2.20289635658 cacc 0.15625\n",
      "i 937 dloss 0.491958200932 gen_x mean 0.423059 closs 2.25724816322 cacc 0.1875\n",
      "i 938 dloss 0.492064714432 gen_x mean 0.419963 closs 2.23316907883 cacc 0.234375\n",
      "i 940 dloss 0.491065740585 gen_x mean 0.425436 closs 2.06035757065 cacc 0.375\n",
      "i 977 dloss 0.477143108845 gen_x mean 0.437074 closs 2.16177964211 cacc 0.25\n",
      "i 990 dloss 0.492367506027 gen_x mean 0.431181 closs 2.16230511665 cacc 0.25\n",
      "i 996 dloss 0.497120141983 gen_x mean 0.427204 closs 2.1641740799 cacc 0.21875\n",
      "all test data\n",
      "test accuracy 0.200848765432\n",
      "all train set\n",
      "DDDDDDZZZZZZZ starting training! using svm linear C 1.0 squared hinge\n",
      "predicting held out\n",
      "###d# 1000 testacc 0.147314297125 trainacc 0.7275390625\n",
      "ZZZZZZ starting training! using svm linear C 1.0 squared hinge\n",
      "predicting held out\n",
      "###dz# 1000 testacc 0.124800319489 trainacc 0.4306640625\n",
      "wrote stats.pkl 176.046452045 total time to run\n",
      "i 1017 dloss 0.497446238995 gen_x mean 0.427367 closs 2.19054269791 cacc 0.265625\n",
      "i 1018 dloss 0.492806851864 gen_x mean 0.431734 closs 2.24036049843 cacc 0.171875\n",
      "i 1026 dloss 0.500310122967 gen_x mean 0.436827 closs 2.20338201523 cacc 0.1875\n",
      "i 1030 dloss 0.498682111502 gen_x mean 0.432235 closs 2.24703574181 cacc 0.140625\n",
      "i 1031 dloss 0.496199190617 gen_x mean 0.427246 closs 2.19429373741 cacc 0.25\n",
      "i 1047 dloss 0.490417778492 gen_x mean 0.424281 closs 2.27344202995 cacc 0.1875\n",
      "i 1051 dloss 0.50023651123 gen_x mean 0.430568 closs 2.23671793938 cacc 0.171875\n",
      "i 1053 dloss 0.500297546387 gen_x mean 0.428315 closs 2.31321334839 cacc 0.125\n",
      "i 1059 dloss 0.501044869423 gen_x mean 0.416938 closs 2.24432730675 cacc 0.171875\n",
      "i 1067 dloss 0.493396937847 gen_x mean 0.435261 closs 2.21474647522 cacc 0.21875\n",
      "i 1068 dloss 0.492090433836 gen_x mean 0.435846 closs 2.23061561584 cacc 0.1875\n",
      "i 1076 dloss 0.497078061104 gen_x mean 0.435726 closs 2.31787705421 cacc 0.109375\n",
      "i 1083 dloss 0.491837859154 gen_x mean 0.435541 closs 2.25283718109 cacc 0.1875\n",
      "i 1093 dloss 0.505911171436 gen_x mean 0.439753 closs 2.24127650261 cacc 0.1875\n",
      "i 1104 dloss 0.493139714003 gen_x mean 0.437833 closs 2.20678830147 cacc 0.21875\n",
      "i 1123 dloss 0.494221270084 gen_x mean 0.429377 closs 2.10174202919 cacc 0.265625\n",
      "i 1128 dloss 0.512649714947 gen_x mean 0.437704 closs 2.21619963646 cacc 0.21875\n",
      "i 1136 dloss 0.496342420578 gen_x mean 0.448072 closs 2.32064676285 cacc 0.109375\n",
      "i 1147 dloss 0.492148220539 gen_x mean 0.426517 closs 2.22925615311 cacc 0.15625\n",
      "i 1152 dloss 0.510292649269 gen_x mean 0.422663 closs 2.14627742767 cacc 0.1875\n",
      "i 1158 dloss 0.504775285721 gen_x mean 0.44235 closs 2.16481781006 cacc 0.296875\n",
      "i 1168 dloss 0.492018550634 gen_x mean 0.42939 closs 2.39294958115 cacc 0.0625\n",
      "i 1176 dloss 0.496899068356 gen_x mean 0.432339 closs 2.26932287216 cacc 0.171875\n",
      "i 1177 dloss 0.495681494474 gen_x mean 0.430679 closs 2.25640392303 cacc 0.109375\n",
      "i 1187 dloss 0.517752528191 gen_x mean 0.426002 closs 2.18779301643 cacc 0.21875\n",
      "i 1197 dloss 0.494235217571 gen_x mean 0.430585 closs 2.25281500816 cacc 0.171875\n",
      "i 1204 dloss 0.486942857504 gen_x mean 0.420033 closs 2.14296770096 cacc 0.28125\n",
      "i 1208 dloss 0.48469671607 gen_x mean 0.430041 closs 2.21694135666 cacc 0.1875\n",
      "i 1211 dloss 0.493508756161 gen_x mean 0.444561 closs 2.25764012337 cacc 0.09375\n",
      "i 1269 dloss 0.480740487576 gen_x mean 0.44244 closs 2.21182584763 cacc 0.21875\n",
      "i 1273 dloss 0.533735692501 gen_x mean 0.450782 closs 2.14722585678 cacc 0.125\n",
      "i 1285 dloss 0.494501322508 gen_x mean 0.429652 closs 2.28855586052 cacc 0.1875\n",
      "i 1286 dloss 0.491405487061 gen_x mean 0.422207 closs 2.15721607208 cacc 0.234375\n",
      "i 1318 dloss 0.490335345268 gen_x mean 0.437727 closs 2.21478867531 cacc 0.15625\n",
      "i 1323 dloss 0.488693445921 gen_x mean 0.43729 closs 2.20503973961 cacc 0.1875\n",
      "i 1355 dloss 0.508467197418 gen_x mean 0.457592 closs 2.30279946327 cacc 0.09375\n",
      "i 1363 dloss 0.494652718306 gen_x mean 0.443627 closs 2.25662946701 cacc 0.15625\n",
      "i 1367 dloss 0.497210085392 gen_x mean 0.436757 closs 2.17413139343 cacc 0.234375\n",
      "i 1381 dloss 0.514323413372 gen_x mean 0.446331 closs 2.22460818291 cacc 0.234375\n",
      "i 1388 dloss 0.526673257351 gen_x mean 0.42625 closs 2.18016195297 cacc 0.234375\n",
      "i 1419 dloss 0.490746974945 gen_x mean 0.423982 closs 2.32617068291 cacc 0.15625\n",
      "i 1430 dloss 0.500340640545 gen_x mean 0.443002 closs 2.24183750153 cacc 0.21875\n",
      "i 1438 dloss 0.498201340437 gen_x mean 0.430228 closs 2.25874042511 cacc 0.15625\n",
      "i 1450 dloss 0.497915685177 gen_x mean 0.411176 closs 2.19333124161 cacc 0.28125\n",
      "i 1492 dloss 0.508862733841 gen_x mean 0.430043 closs 2.24847388268 cacc 0.171875\n",
      "i 1505 dloss 0.5014950037 gen_x mean 0.441312 closs 2.21281886101 cacc 0.125\n",
      "i 1507 dloss 0.492626786232 gen_x mean 0.447958 closs 2.1647772789 cacc 0.21875\n",
      "i 1515 dloss 0.506259918213 gen_x mean 0.44232 closs 2.21622419357 cacc 0.171875\n",
      "i 1518 dloss 0.496209859848 gen_x mean 0.433999 closs 2.2652695179 cacc 0.125\n",
      "i 1529 dloss 0.494645118713 gen_x mean 0.44911 closs 2.28675913811 cacc 0.234375\n",
      "i 1530 dloss 0.485750198364 gen_x mean 0.450368 closs 2.3031642437 cacc 0.140625\n",
      "i 1552 dloss 0.518515467644 gen_x mean 0.429421 closs 2.28070282936 cacc 0.15625\n",
      "i 1565 dloss 0.500293791294 gen_x mean 0.450899 closs 2.26713991165 cacc 0.171875\n",
      "i 1569 dloss 0.483989328146 gen_x mean 0.467077 closs 2.17206168175 cacc 0.1875\n",
      "i 1571 dloss 0.525090575218 gen_x mean 0.462163 closs 2.2027900219 cacc 0.15625\n",
      "i 1580 dloss 0.523288607597 gen_x mean 0.45496 closs 2.30030179024 cacc 0.125\n",
      "i 1582 dloss 0.497072219849 gen_x mean 0.459271 closs 2.17604899406 cacc 0.171875\n",
      "i 1584 dloss 0.479941368103 gen_x mean 0.445742 closs 2.20955514908 cacc 0.15625\n",
      "i 1586 dloss 0.536082386971 gen_x mean 0.427884 closs 2.12768650055 cacc 0.1875\n",
      "i 1590 dloss 0.495480000973 gen_x mean 0.444993 closs 2.17149281502 cacc 0.25\n",
      "i 1596 dloss 0.495557308197 gen_x mean 0.446891 closs 2.12490773201 cacc 0.203125\n",
      "i 1599 dloss 0.505303025246 gen_x mean 0.422193 closs 2.29595422745 cacc 0.171875\n",
      "i 1604 dloss 0.496955245733 gen_x mean 0.439686 closs 2.23199176788 cacc 0.140625\n",
      "i 1620 dloss 0.493403851986 gen_x mean 0.465575 closs 2.20027184486 cacc 0.1875\n",
      "i 1643 dloss 0.501009464264 gen_x mean 0.43471 closs 2.26388883591 cacc 0.140625\n",
      "i 1649 dloss 0.499117702246 gen_x mean 0.423155 closs 2.23640012741 cacc 0.15625\n",
      "i 1655 dloss 0.505533099174 gen_x mean 0.4466 closs 2.16471076012 cacc 0.21875\n",
      "i 1682 dloss 0.500540792942 gen_x mean 0.442284 closs 2.15097165108 cacc 0.234375\n",
      "i 1684 dloss 0.516673564911 gen_x mean 0.427256 closs 2.22722673416 cacc 0.171875\n",
      "i 1703 dloss 0.490588724613 gen_x mean 0.42954 closs 2.24419379234 cacc 0.15625\n",
      "i 1716 dloss 0.48636290431 gen_x mean 0.436498 closs 2.11779522896 cacc 0.25\n",
      "i 1725 dloss 0.503886818886 gen_x mean 0.437817 closs 2.29538464546 cacc 0.09375\n",
      "i 1727 dloss 0.484620690346 gen_x mean 0.435468 closs 2.30408644676 cacc 0.09375\n",
      "i 1730 dloss 0.488622367382 gen_x mean 0.425812 closs 2.14882588387 cacc 0.21875\n",
      "i 1734 dloss 0.51884573698 gen_x mean 0.435312 closs 2.28347325325 cacc 0.09375\n",
      "i 1739 dloss 0.533641695976 gen_x mean 0.442623 closs 2.34267163277 cacc 0.15625\n",
      "i 1743 dloss 0.478472650051 gen_x mean 0.432249 closs 2.14153194427 cacc 0.203125\n",
      "i 1774 dloss 0.499245285988 gen_x mean 0.441353 closs 2.20052027702 cacc 0.171875\n",
      "i 1779 dloss 0.5037946105 gen_x mean 0.436477 closs 2.21246123314 cacc 0.234375\n",
      "i 1791 dloss 0.514003634453 gen_x mean 0.448512 closs 2.09573411942 cacc 0.265625\n",
      "i 1804 dloss 0.502429127693 gen_x mean 0.435753 closs 2.20212340355 cacc 0.21875\n",
      "i 1807 dloss 0.493818104267 gen_x mean 0.441377 closs 2.27909874916 cacc 0.1875\n",
      "i 1820 dloss 0.486873835325 gen_x mean 0.440765 closs 2.13777756691 cacc 0.25\n",
      "i 1846 dloss 0.481888145208 gen_x mean 0.446047 closs 2.30879497528 cacc 0.15625\n",
      "i 1854 dloss 0.531399607658 gen_x mean 0.446511 closs 2.26117229462 cacc 0.15625\n",
      "i 1872 dloss 0.503381371498 gen_x mean 0.446451 closs 2.1638906002 cacc 0.140625\n",
      "i 1873 dloss 0.508175730705 gen_x mean 0.448355 closs 2.26851224899 cacc 0.203125\n",
      "i 1875 dloss 0.489417523146 gen_x mean 0.450305 closs 2.22947049141 cacc 0.1875\n",
      "i 1876 dloss 0.512131392956 gen_x mean 0.450861 closs 2.25374984741 cacc 0.1875\n",
      "i 1878 dloss 0.498602092266 gen_x mean 0.452895 closs 2.26201200485 cacc 0.1875\n",
      "i 1885 dloss 0.502841114998 gen_x mean 0.443732 closs 2.2325232029 cacc 0.203125\n",
      "i 1894 dloss 0.496129870415 gen_x mean 0.446447 closs 2.21566534042 cacc 0.140625\n",
      "i 1903 dloss 0.514230728149 gen_x mean 0.438412 closs 2.16110754013 cacc 0.21875\n",
      "i 1904 dloss 0.498271435499 gen_x mean 0.437491 closs 2.10192060471 cacc 0.265625\n",
      "i 1905 dloss 0.483191937208 gen_x mean 0.43796 closs 2.23523974419 cacc 0.171875\n",
      "i 1935 dloss 0.50526291132 gen_x mean 0.432688 closs 2.20892596245 cacc 0.203125\n",
      "i 1941 dloss 0.493329018354 gen_x mean 0.434873 closs 2.22050023079 cacc 0.234375\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(0,500000):\n",
    "\n",
    "    z_in2 = rng.normal(size=(64,nl)).astype('float32')\n",
    "\n",
    "    if latent_sparse:\n",
    "        z_in2[:,128:] *= 0.0\n",
    "\n",
    "    r = random.randint(0,num_examples-64)\n",
    "\n",
    "    if dataset == \"mnist\":\n",
    "        x_in = trainx[r:r+64]\n",
    "        y_in = trainy[r:r+64]\n",
    "\n",
    "        x_in = x_in.reshape((64,1,28,28))\n",
    "\n",
    "        x_in = np.repeat(x_in,3,axis=(1))\n",
    "        x_in = np.lib.pad(x_in,((0,0),(0,0),(2,2),(2,2)),'constant',constant_values=(0))\n",
    "\n",
    "        x_in = x_in.reshape((64,32*32*3))\n",
    "    elif dataset == \"anime\":\n",
    "        x_in = normalize(animeData.getBatch()).reshape((64,32*32*3))\n",
    "\n",
    "    elif dataset == \"svhn\":\n",
    "\n",
    "        #just do a quick update using the whole thing.  \n",
    "\n",
    "        ind = random.randint(0,574168-64)\n",
    "        svhn_batch = svhnData.getBatch(mb_size=64,index=ind,segment=\"train\")\n",
    "        x_in2 = normalize(svhn_batch['x']).reshape((64,32*32*3))\n",
    "        train_disc_gen_faster(x_in2,z_in2)\n",
    "\n",
    "        if random.uniform(0,1) < 0.1:\n",
    "            ind = random.randint(0,num_labeled_examples_use)\n",
    "            svhn_batch = svhnData.getBatch(mb_size=64,index=ind,segment=\"hard_train\")\n",
    "            x_in2 = normalize(svhn_batch['x']).reshape((64,32*32*3))\n",
    "            y_in = svhn_batch['y']\n",
    "\n",
    "            dloss,gen_x,z_out_p,closs,cacc = train_disc_gen_classifier(x_in2,z_in2,y_in)\n",
    "\n",
    "            print \"i\", iteration, \"dloss\", dloss, \"gen_x mean\", gen_x.mean(), \"closs\", closs, \"cacc\", cacc\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "\n",
    "        if dataset == \"svhn\":\n",
    "            acclst = []\n",
    "            for ind in range(0,26032-128,64):\n",
    "            #ind = random.randint(0,26032-64)\n",
    "                testbatch = svhnData.getBatch(index=ind,mb_size=64,segment=\"test\")\n",
    "                closs,cacc = test_classifier(normalize(testbatch['x']).reshape((64,32*32*3)),testbatch['y'])\n",
    "                acclst.append(cacc)\n",
    "\n",
    "            print \"all test data\"\n",
    "            print \"test accuracy\", sum(acclst)*1.0/len(acclst)\n",
    "\n",
    "        #######plot_images(gen_x, \"plots/\" + slurm_name + \"_gen.png\")\n",
    "        plot_images(func_z_to_x(func_x_to_z(x_in2)).reshape((64,32*32*3)), \"plots/\" + slurm_name + \"_rec.png\")\n",
    "\n",
    "\n",
    "\n",
    "        dhlst = []\n",
    "        dzhlst = []\n",
    "        ylst = []\n",
    "        t0 = time.time()\n",
    "\n",
    "        dhlst_test = []\n",
    "        dzhlst_test = []\n",
    "        ylst_test = []\n",
    "\n",
    "        print \"all train set\"\n",
    "#         print \"only using eoz features\"\n",
    "\n",
    "        for ind in range(0,1000,64):\n",
    "\n",
    "            svhn_batch = svhnData.getBatch(mb_size=64,index=ind,segment=\"train\")\n",
    "            x = normalize(svhn_batch['x']).reshape((64,3*32*32))\n",
    "            y = svhn_batch['y']\n",
    "\n",
    "            zstuff = get_z(x)\n",
    "            dzstuff = get_dz(x)\n",
    "\n",
    "            #print zstuff[0].shape, zstuff[1].shape, zstuff[2].shape\n",
    "            dhlst.append(zstuff)\n",
    "            dzhlst.append(np.concatenate(dzstuff,axis=1))\n",
    "            \n",
    "            ylst.append(y)\n",
    "\n",
    "        for ind in range(0,20000,64):\n",
    "            svhn_batch = svhnData.getBatch(mb_size=64,index=ind,segment=\"test\")\n",
    "            x = normalize(svhn_batch['x']).reshape((64,3*32*32))\n",
    "            y = svhn_batch['y']\n",
    "            zstuff = get_z(x)\n",
    "            dzstuff = get_dz(x)\n",
    "            \n",
    "            dhlst_test.append(zstuff)\n",
    "            dzhlst_test.append(np.concatenate(dzstuff,axis=1))\n",
    "            \n",
    "            ylst_test.append(y)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        ##### DDDDDDZZZZZZZ\n",
    "        X_train = np.vstack(dzhlst)\n",
    "        Y_train = np.vstack(ylst).flatten()\n",
    "\n",
    "        X_test = np.vstack(dzhlst_test)\n",
    "        Y_test = np.vstack(ylst_test).flatten()\n",
    "\n",
    "        \n",
    "        C = 1.0\n",
    "        print \"DDDDDDZZZZZZZ starting training!\", \"using svm linear\", \"C\", C, \"squared hinge\"\n",
    "\n",
    "        model = svm.LinearSVC(C=C,loss='squared_hinge')\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        print \"predicting held out\"\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        testacc = np.mean(np.equal(Y_test, y_pred))\n",
    "        #print \"held out accuracy\", testacc\n",
    "\n",
    "        y_pred = model.predict(X_train)\n",
    "\n",
    "        trainacc = np.mean(np.equal(Y_train, y_pred))\n",
    "        #print \"training accuracy\", trainacc\n",
    "\n",
    "        #print time.time() - t0, \"total time to run\"\n",
    "        \n",
    "        print \"###d#\",iteration, \"testacc\", testacc, \"trainacc\", trainacc\n",
    "        zstats.append([testacc, trainacc])\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##### ZZZZZZZ\n",
    "        \n",
    "        X_train = np.vstack(dhlst)\n",
    "        Y_train = np.vstack(ylst).flatten()\n",
    "\n",
    "        X_test = np.vstack(dhlst_test)\n",
    "        Y_test = np.vstack(ylst_test).flatten()\n",
    "\n",
    "\n",
    "        C = 1.0\n",
    "        print \"ZZZZZZ starting training!\", \"using svm linear\", \"C\", C, \"squared hinge\"\n",
    "\n",
    "        model = svm.LinearSVC(C=C,loss='squared_hinge')\n",
    "\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        print \"predicting held out\"\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        testacc = np.mean(np.equal(Y_test, y_pred))\n",
    "        #print \"held out accuracy\", testacc\n",
    "\n",
    "        y_pred = model.predict(X_train)\n",
    "\n",
    "        trainacc = np.mean(np.equal(Y_train, y_pred))\n",
    "        #print \"training accuracy\", trainacc\n",
    "\n",
    "        \n",
    "        print \"###dz#\",iteration, \"testacc\", testacc, \"trainacc\", trainacc\n",
    "        dzstats.append([testacc, trainacc])\n",
    "        \n",
    "        statsname = \"stats.pkl\"\n",
    "        pickle.dump([zstats,dzstats], open(statsname, \"w\", 0))\n",
    "        print \"wrote\", statsname, time.time() - t0, \"total time to run\"\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.asarray(dzstats)[:,1], label=\"Disc Train\", linestyle='--')\n",
    "\n",
    "plt.plot(np.asarray(dzstats)[:,0], label=\"Disc Test\", linestyle='--')\n",
    "\n",
    "# plt.plot(np.asarray(zstats)[:,1], label=\"Train\", linestyle='-')\n",
    "# plt.plot(np.asarray(zstats)[:,0], label=\"Test\", linestyle='-')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
