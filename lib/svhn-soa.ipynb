{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5105 on context None\n",
      "Mapped name None to device cuda0: Quadro K6000 (0000:04:00.0)\n",
      "/Tmp/lisa/os_v5/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Tmp/lisa/os_v5/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python \n",
    "\n",
    "'''\n",
    "-Initially make z_to_x and x_to_z fairly shallow networks.  Inject noise?  \n",
    "\n",
    "-Use the fflayer class?  \n",
    "\n",
    "'''\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/u/lambalex/DeepLearning/undirected_matching\")\n",
    "sys.path.append(\"/u/lambalex/DeepLearning/undirected_matching/lib\")\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from nn_layers import fflayer, param_init_fflayer, param_init_convlayer, convlayer\n",
    "from utils import init_tparams, join2, srng, dropout, inverse_sigmoid, join3, merge_images\n",
    "from loss import accuracy, crossent, lsgan_loss, wgan_loss, improvement_loss\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "import random\n",
    "from viz import plot_images\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "slurm_name = os.environ[\"SLURM_JOB_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (574168, 3, 32, 32)\n",
      "svhn shape (574168, 3, 32, 32)\n",
      "0 255\n",
      "num latent 128\n",
      "dataset svhn\n",
      "num steps 1\n",
      "train classifier separate True\n",
      "latent sparse False\n",
      "persistent p chain False\n",
      "blending rate (odds of keeping old z in P chain) 0.5\n",
      "improvement loss weight 0.0\n"
     ]
    }
   ],
   "source": [
    "class ConsiderConstant(theano.compile.ViewOp):\n",
    "    def grad(self, args, g_outs):\n",
    "        return [T.zeros_like(g_out) for g_out in g_outs]\n",
    "\n",
    "consider_constant = ConsiderConstant()\n",
    "\n",
    "#dataset = \"mnist\"\n",
    "#dataset = \"anime\"\n",
    "dataset = \"svhn\"\n",
    "\n",
    "if dataset == \"mnist\":\n",
    "    mn = gzip.open(\"/u/lambalex/data/mnist/mnist.pkl.gz\")\n",
    "\n",
    "    train, valid, test = pickle.load(mn)\n",
    "\n",
    "    trainx,trainy = train\n",
    "    \n",
    "    \n",
    "    #newtx = trainx[(trainy<2) | (trainy>8)]\n",
    "    #newty = trainy[(trainy<2) | (trainy>8)]\n",
    "    #trainx = newtx\n",
    "    #trainy = newty\n",
    "    \n",
    "    validx,validy = valid\n",
    "    testx, testy = test\n",
    "\n",
    "    num_examples = trainx.shape[0]\n",
    "\n",
    "    m = 784\n",
    "elif dataset == \"anime\":\n",
    "    from load_file import FileData, normalize, denormalize\n",
    "\n",
    "    loc = \"/u/lambalex/DeepLearning/animefaces/datafaces/danbooru-faces/\"\n",
    "\n",
    "    animeData = FileData(loc, 32, 64)\n",
    "\n",
    "    m = 32*32*3\n",
    "\n",
    "elif dataset == \"svhn\":\n",
    "\n",
    "    from load_svhn import SvhnData\n",
    "    from load_file import normalize, denormalize\n",
    "\n",
    "    svhnData = SvhnData(mb_size=64,segment=\"train\")\n",
    "\n",
    "    num_examples = 50000\n",
    "\n",
    "nl = 128\n",
    "print \"num latent\", nl\n",
    "#128 works for nl\n",
    "nfg = 512\n",
    "nfd = 512\n",
    "\n",
    "print \"dataset\", dataset\n",
    "\n",
    "#3\n",
    "num_steps = 1\n",
    "print \"num steps\", num_steps\n",
    "\n",
    "train_classifier_separate = True\n",
    "print \"train classifier separate\", train_classifier_separate\n",
    "\n",
    "#skip_conn = True\n",
    "#print \"skip conn\", skip_conn\n",
    "\n",
    "latent_sparse = False\n",
    "print \"latent sparse\", latent_sparse\n",
    "\n",
    "persist_p_chain = False\n",
    "print \"persistent p chain\", persist_p_chain\n",
    "\n",
    "blending_rate = 0.5\n",
    "print 'blending rate (odds of keeping old z in P chain)', blending_rate\n",
    "\n",
    "improvement_loss_weight = 0.0\n",
    "print \"improvement loss weight\", improvement_loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_gparams(p):\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='z_x_1',nin=nl*2,nout=512*4*4,ortho=False,batch_norm=True)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_2',nin=512,nout=256,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_3',nin=256*1,nout=128,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_4',nin=128*1,nout=3,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_1',nin=3,nout=128,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_2',nin=128,nout=256,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_3',nin=256,nout=512,kernel_len=5,batch_norm=True)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_mu',nin=512*4*4,nout=nl,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_sigma',nin=512*4*4,nout=nl,ortho=False,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "def init_dparams(p):\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_1',nin=3,nout=128,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_2',nin=128,nout=256,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_3',nin=256,nout=512,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_1',nin=nl+512*4*4,nout=nfd,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_2',nin=nfd,nout=nfd,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_3',nin=nfd,nout=nfd,ortho=False,batch_norm=False)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_1',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_2',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_3',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_4',nin=128,nout=1,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_5',nin=256,nout=1,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_6',nin=512,nout=1,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "\n",
    "def z_to_x(p,z):\n",
    "\n",
    "    print \"extra noise input\"\n",
    "    z_inp = join2(z, 1.0*srng.normal(size=z.shape))\n",
    "\n",
    "    d0 = fflayer(tparams=p,state_below=z_inp,options={},prefix='z_x_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    d0 = d0.reshape((64,512,4,4))\n",
    "\n",
    "    d1 = convlayer(tparams=p,state_below=d0,options={},prefix='z_x_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=-2)\n",
    "\n",
    "    d2 = convlayer(tparams=p,state_below=d1,options={},prefix='z_x_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=-2)\n",
    "\n",
    "    d3 = convlayer(tparams=p,state_below=d2,options={},prefix='z_x_4',activ='lambda x: x',stride=-2)\n",
    "\n",
    "    x_new = d3.flatten(2)\n",
    "\n",
    "    return x_new\n",
    "\n",
    "def x_to_z(p,x):\n",
    "\n",
    "    e1 = convlayer(tparams=p,state_below=x.reshape((64,3,32,32)),options={},prefix='x_z_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    e2 = convlayer(tparams=p,state_below=e1,options={},prefix='x_z_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    e3 = convlayer(tparams=p,state_below=e2,options={},prefix='x_z_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    eo = e3\n",
    "    eo = eo.flatten(2)\n",
    "\n",
    "    sigma = fflayer(tparams=p,state_below=eo,options={},prefix='x_z_mu',activ='lambda x: x')\n",
    "    mu = fflayer(tparams=p,state_below=eo,options={},prefix='x_z_sigma',activ='lambda x: x')\n",
    "\n",
    "    eps = srng.normal(size=sigma.shape)\n",
    "\n",
    "    z_new = eps*T.nnet.sigmoid(sigma) + mu\n",
    "    print \"turned on injected noise in x->z connection\"\n",
    "\n",
    "    z_new = (z_new - T.mean(z_new, axis=0, keepdims=True)) / (0.001 + T.std(z_new, axis=0, keepdims=True))\n",
    "\n",
    "    return z_new, eo\n",
    "\n",
    "\n",
    "def discriminator(p,x,z):\n",
    "\n",
    "    dc_1 = convlayer(tparams=p,state_below=x.reshape((64,3,32,32)),options={},prefix='DC_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    dc_2 = convlayer(tparams=p,state_below=dc_1,options={},prefix='DC_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    dc_3 = convlayer(tparams=p,state_below=dc_2,options={},prefix='DC_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    inp = join2(z,dc_3.flatten(2))\n",
    "\n",
    "    h1 = fflayer(tparams=p,state_below=inp,options={},prefix='D_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',mean_ln=False)\n",
    "\n",
    "    h2 = fflayer(tparams=p,state_below=h1,options={},prefix='D_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)', mean_ln=False)\n",
    "\n",
    "    h3 = fflayer(tparams=p,state_below=h2,options={},prefix='D_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)', mean_ln=False)\n",
    "\n",
    "    D1 = fflayer(tparams=p,state_below=h1,options={},prefix='D_o_1',activ='lambda x: x')\n",
    "    D2 = fflayer(tparams=p,state_below=h2,options={},prefix='D_o_2',activ='lambda x: x')\n",
    "    D3 = fflayer(tparams=p,state_below=h3,options={},prefix='D_o_3',activ='lambda x: x')\n",
    "\n",
    "    D4 = convlayer(tparams=p,state_below=dc_1,options={},prefix='D_o_4',activ='lambda x: x',stride=2)\n",
    "    D5 = convlayer(tparams=p,state_below=dc_2,options={},prefix='D_o_5',activ='lambda x: x',stride=2)\n",
    "    D6 = convlayer(tparams=p,state_below=dc_3,options={},prefix='D_o_6',activ='lambda x: x',stride=2)\n",
    "\n",
    "    print \"special thing in D\"\n",
    "    return [D1,D2,D3,D4,D5,D6], h3\n",
    "\n",
    "def p_chain(p, z, num_iterations):\n",
    "    zlst = [z]\n",
    "    xlst = []\n",
    "\n",
    "    if num_iterations == 1:\n",
    "        \n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        #new_z = x_to_z(p, xlst[-1])\n",
    "        #zlst.append(new_z)\n",
    "\n",
    "    elif num_iterations == 3:  \n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        new_z = x_to_z(p, consider_constant(xlst[-1]))\n",
    "        zlst.append(new_z)\n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        new_z = x_to_z(p, consider_constant(xlst[-1]))\n",
    "        zlst.append(new_z)\n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "\n",
    "    else:\n",
    "\n",
    "        for inds in range(0,num_iterations):\n",
    "            new_x = z_to_x(p, zlst[-1])\n",
    "            xlst.append(new_x)\n",
    "            new_z, eo = x_to_z(p, xlst[-1])\n",
    "            zlst.append(new_z)\n",
    "\n",
    "\n",
    "    for j in range(len(xlst)):\n",
    "        xlst[j] = T.nnet.sigmoid(xlst[j])\n",
    "\n",
    "    return xlst, zlst\n",
    "\n",
    "def onestep_z_to_x(p,z):\n",
    "    x = T.nnet.sigmoid(z_to_x(p, z))\n",
    "    return x\n",
    "\n",
    "def onestep_x_to_z(p,x):\n",
    "    new_z, eo = x_to_z(p, inverse_sigmoid(x))\n",
    "    return new_z, eo\n",
    "\n",
    "def q_chain(p,x,num_iterations):\n",
    "\n",
    "    xlst = [x]\n",
    "    zlst = []\n",
    "    new_z, eo = x_to_z(p, inverse_sigmoid(xlst[-1]))\n",
    "    zlst.append(new_z)\n",
    "\n",
    "    return xlst, zlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "directory = \"network-temp/\"\n",
    "ext = \"svhn-soa.p\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "def save_network(gparams,dparams,name):\n",
    "    pkl_params = (gparams,dparams)\n",
    "    out = open(directory + str(name) + ext, \"w\", 0) #bufsize=0\n",
    "    pickle.dump(pkl_params, out)\n",
    "    out.close()\n",
    "\n",
    "def load_network(name):\n",
    "    return pickle.load(open(directory + str(name) + ext, \"r\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gparams = init_gparams({})\n",
    "# dparams = init_dparams({})\n",
    "\n",
    "gparams,dparams = load_network(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra noise input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Quadro K6000 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "[sigmoid.0]\n",
      "[z_in]\n",
      "[<TensorType(float32, matrix)>]\n",
      "[Elemwise{true_div,no_inplace}.0]\n",
      "special thing in D\n",
      "special thing in D\n",
      "single disc\n",
      "not using improvement objective\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n"
     ]
    }
   ],
   "source": [
    "z_in = T.matrix('z_in')\n",
    "x_in = T.matrix()\n",
    "\n",
    "p_lst_x,p_lst_z = p_chain(gparams, z_in, num_steps)\n",
    "\n",
    "q_lst_x,q_lst_z = q_chain(gparams, x_in, num_steps)\n",
    "\n",
    "p_lst_x_long,p_lst_z_long = p_chain(gparams, z_in, 19)\n",
    "\n",
    "z_inf = q_lst_z[-1]\n",
    "\n",
    "print p_lst_x\n",
    "print p_lst_z\n",
    "print q_lst_x\n",
    "print q_lst_z\n",
    "\n",
    "#D_p_lst_3,_ = discriminator(dparams, p_lst_x[2], p_lst_z[2])\n",
    "\n",
    "#D_p_lst_2,_ = discriminator(dparams, p_lst_x[1], p_lst_z[1])\n",
    "\n",
    "D_p_lst_1,_ = discriminator(dparams, p_lst_x[0], p_lst_z[0])\n",
    "\n",
    "D_q_lst,D_feat_q = discriminator(dparams, q_lst_x[-1], q_lst_z[-1])\n",
    "\n",
    "dloss, gloss = lsgan_loss(D_q_lst, D_p_lst_1)\n",
    "\n",
    "print \"single disc\"\n",
    "print \"not using improvement objective\"\n",
    "#improvement_objective = improvement_loss_weight * improvement_loss(D_p_lst_1, D_p_lst_2)\n",
    "#gloss += improvement_objective\n",
    "\n",
    "lr = theano.shared(np.array(0.0001, dtype=theano.config.floatX))\n",
    "\n",
    "dupdates = lasagne.updates.rmsprop(dloss, dparams.values(),lr)\n",
    "gloss_grads = T.grad(gloss, gparams.values(), disconnected_inputs='ignore')\n",
    "gupdates = lasagne.updates.rmsprop(gloss_grads, gparams.values(),lr)\n",
    "\n",
    "gcupdates = lasagne.updates.rmsprop(gloss, gparams.values(),lr)\n",
    "\n",
    "dgupdates = dupdates.copy()\n",
    "dgupdates.update(gupdates)\n",
    "\n",
    "dgcupdates = dupdates.copy()\n",
    "dgcupdates.update(gcupdates)\n",
    "\n",
    "train_disc_gen_classifier = theano.function(inputs = [x_in, z_in], outputs=[dloss,p_lst_x[-1],p_lst_z[-1]], updates=dgcupdates,on_unused_input='ignore')\n",
    "\n",
    "train_disc_gen_classifier_fast = theano.function(inputs = [x_in, z_in], outputs=[dloss], updates=dgcupdates,on_unused_input='ignore')\n",
    "\n",
    "\n",
    "get_zinf = theano.function([x_in], outputs=z_inf)\n",
    "#get_dfeat = theano.function([x_in], outputs=D_feat_q)\n",
    "\n",
    "#get_pchain = theano.function([z_in], outputs = p_lst_x_long)\n",
    "\n",
    "x_in = T.matrix()\n",
    "\n",
    "func_z_to_x = theano.function([z_in], outputs = onestep_z_to_x(gparams, z_in))\n",
    "func_x_to_z = theano.function([x_in], outputs = onestep_x_to_z(gparams, x_in))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr.set_value(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 dloss [array(0.4041844308376312, dtype=float32)] gen_x mean"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gen_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5f8e75a1bc18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mprint\u001b[0m \u001b[1;34m\"iteration\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dloss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gen_x mean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gen_x' is not defined"
     ]
    }
   ],
   "source": [
    "z_out_p = rng.normal(size=(64,nl)).astype('float32')\n",
    "\n",
    "for iteration in range(0,500000):\n",
    "\n",
    "    if persist_p_chain:\n",
    "        z_in_new = rng.normal(size=(64,nl)).astype('float32')\n",
    "        blending = rng.uniform(0.0,1.0,size=(64,))\n",
    "        z_in_new[blending>=blending_rate] = z_out_p[blending>=blending_rate]\n",
    "        z_in = z_in_new\n",
    "    else:\n",
    "        z_in = rng.normal(size=(64,nl)).astype('float32')\n",
    "\n",
    "    if latent_sparse:\n",
    "        z_in[:,128:] *= 0.0\n",
    "\n",
    "    r = random.randint(0,num_examples-64)\n",
    "\n",
    "    if dataset == \"mnist\":\n",
    "        x_in = trainx[r:r+64]\n",
    "        y_in = trainy[r:r+64]\n",
    "\n",
    "        x_in = x_in.reshape((64,1,28,28))\n",
    "\n",
    "        x_in = np.repeat(x_in,3,axis=(1))\n",
    "        x_in = np.lib.pad(x_in,((0,0),(0,0),(2,2),(2,2)),'constant',constant_values=(0))\n",
    "\n",
    "        x_in = x_in.reshape((64,32*32*3))\n",
    "    elif dataset == \"anime\":\n",
    "        x_in = normalize(animeData.getBatch()).reshape((64,32*32*3))\n",
    "\n",
    "    elif dataset == \"svhn\":\n",
    "        x_in = normalize(svhnData.getBatch()['x']).reshape((64,32*32*3))\n",
    "\n",
    "    #dloss,gen_x,z_out_p = train_disc_gen_classifier(x_in,z_in)\n",
    "    \n",
    "    dloss= train_disc_gen_classifier_fast(x_in,z_in)\n",
    "\n",
    "\n",
    "    if iteration % 10 == 0:\n",
    "        print \"iteration\", iteration, \"dloss\", dloss, \"gen_x mean\", gen_x.mean()\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        print \"dloss\", dloss\n",
    "        dloss,gen_x,z_out_p = train_disc_gen_classifier(x_in,z_in)\n",
    "        plot_images(gen_x, \"plots/\" + slurm_name + \"_gen.png\")\n",
    "        \n",
    "        \n",
    "        print \"saving\"\n",
    "        save_network(gparams,dparams,\"test\")\n",
    "        \n",
    "        #plot_images(reconstruct(x_in).reshape((64,1,28,28)), \"plots/\" + slurm_name + \"_rec.png\")\n",
    "\n",
    "        #NOT CORRECT INITIALLY\n",
    "        #rec_loop = [x_in]\n",
    "        #for b in range(0,9):\n",
    "        #    rec_loop.append(reconstruct(rec_loop[-1]))\n",
    "        #    rec_loop[-1][:,0:392] = x_in[:,0:392]\n",
    "        #    plot_images(rec_loop[-1].reshape((64,1,28,28)), \"plots/\" + slurm_name + \"_rec_\" + str(b) +\".png\")\n",
    "\n",
    "#         plot_images(x_in, \"plots/\" + slurm_name + \"_original.png\")\n",
    "\n",
    "#         #p_chain = get_pchain(z_in)\n",
    "#         new_z = rng.normal(size=(64,nl)).astype('float32')\n",
    "#         for j in range(0,20):\n",
    "#             new_x = func_z_to_x(new_z)\n",
    "#             new_z = func_x_to_z(new_x)\n",
    "#             print \"printing element of p_chain\", j\n",
    "#             plot_images(new_x, \"plots/\" + slurm_name + \"_pchain_\" + str(j) + \".png\")\n",
    "\n",
    "#         new_z = rng.normal(size=(64,nl)).astype('float32')\n",
    "#         for j in range(0,20):\n",
    "#             new_x = func_z_to_x(new_z)\n",
    "#             new_x = merge_images(new_x, x_in)\n",
    "#             new_z = func_x_to_z(new_x)\n",
    "#             plot_images(new_x, \"plots/\" + slurm_name + \"_inpainting_\" + str(j) + \".png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_network(gparams,dparams,\"test-backup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load_network(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svhnData.getBatch()['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svhnData.train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "super_samples 8192\n"
     ]
    }
   ],
   "source": [
    "# super_samples = 2**13\n",
    "# print \"super_samples\", super_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = 1024\n",
    "num_valid = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soa_trainx = svhnData.train_X[:num_train]\n",
    "soa_trainy = svhnData.train_Y[:num_train].reshape(num_train,)\n",
    "\n",
    "soa_validx = svhnData.train_X[num_train:num_train+num_valid]\n",
    "soa_validy = svhnData.train_Y[num_train:num_train+num_valid].reshape(num_valid,)\n",
    "\n",
    "soa_testx = svhnData.train_X[num_train+num_valid:]\n",
    "soa_testy = svhnData.train_Y[num_train+num_valid:].reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soa_trainx (1024, 3, 32, 32)\n",
      "soa_validx (8192, 3, 32, 32)\n",
      "soa_testx (564952, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print \"soa_trainx\", soa_trainx.shape\n",
    "print \"soa_validx\", soa_validx.shape\n",
    "print \"soa_testx\", soa_testx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_batch = soa_trainx[:64]\n",
    "# x_under_test = normalize(x_batch).reshape((64,32*32*3))\n",
    "# z_under_test, zeo_under_test = func_x_to_z(x_under_test)\n",
    "# print \"z_under_test\", z_under_test.shape\n",
    "# print \"zeo_under_test\", zeo_under_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "soa_trainz = np.ones((0,128))\n",
    "soa_trainzeo = np.ones((0,8192))\n",
    "for start in range(0, len(soa_trainx), batch_size):\n",
    "    x_batch = soa_trainx[start:start + batch_size]\n",
    "    x_under_test = normalize(x_batch).reshape((batch_size,32*32*3))\n",
    "    z_under_test, zeo_under_test = func_x_to_z(x_under_test)\n",
    "    soa_trainz = np.vstack([soa_trainz,z_under_test])\n",
    "    soa_trainzeo = np.vstack([soa_trainzeo,zeo_under_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soa_trainz (1024, 128)\n",
      "soa_trainzeo (1024, 8192)\n"
     ]
    }
   ],
   "source": [
    "soa_trainz = soa_trainz.astype(np.float32)\n",
    "soa_trainzeo = soa_trainzeo.astype(np.float32)\n",
    "print \"soa_trainz\", soa_trainz.shape\n",
    "print \"soa_trainzeo\", soa_trainzeo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soa_validz = np.ones((0,128))\n",
    "soa_validzeo = np.ones((0,8192))\n",
    "for start in range(0, len(soa_validx), batch_size):\n",
    "    x_batch = soa_validx[start:start + batch_size]\n",
    "    x_under_test = normalize(x_batch).reshape((batch_size,32*32*3))\n",
    "    z_under_test, zeo_under_test = func_x_to_z(x_under_test)\n",
    "    soa_validz = np.vstack([soa_validz,z_under_test])\n",
    "    soa_validzeo = np.vstack([soa_validzeo,zeo_under_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soa_validz (8192, 128)\n",
      "soa_validzeo (8192, 8192)\n"
     ]
    }
   ],
   "source": [
    "soa_validz = soa_validz.astype(np.float32)\n",
    "soa_validzeo = soa_validzeo.astype(np.float32)\n",
    "print \"soa_validz\", soa_validz.shape\n",
    "print \"soa_validzeo\", soa_validzeo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-6303aad6a2f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mz_under_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeo_under_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_x_to_z\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_under_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msoa_testz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msoa_testz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz_under_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msoa_testzeo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msoa_testzeo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzeo_under_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/Tmp/lisa/os_v5/anaconda/lib/python2.7/site-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "soa_testz = np.ones((0,128))\n",
    "soa_testzeo = np.ones((0,8192))\n",
    "for start in range(0, batch_size*1000, batch_size):\n",
    "    x_batch = soa_testx[start:start + batch_size]\n",
    "    x_under_test = normalize(x_batch).reshape((batch_size,32*32*3))\n",
    "    z_under_test, zeo_under_test = func_x_to_z(x_under_test)\n",
    "    soa_testz = np.vstack([soa_testz,z_under_test])\n",
    "    soa_testzeo = np.vstack([soa_testzeo,zeo_under_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soa_testz (42752, 128)\n",
      "soa_testzeo (42688, 8192)\n"
     ]
    }
   ],
   "source": [
    "soa_testz = soa_testz.astype(np.float32)\n",
    "soa_testzeo = soa_testzeo.astype(np.float32)\n",
    "print \"soa_testz\", soa_testz.shape\n",
    "print \"soa_testzeo\", soa_testzeo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 179,\n",
       "         1: 137,\n",
       "         2: 116,\n",
       "         3: 105,\n",
       "         4: 102,\n",
       "         5: 80,\n",
       "         6: 89,\n",
       "         7: 58,\n",
       "         8: 75,\n",
       "         9: 83})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(soa_trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joelearn.embeddings import tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coor_tsne = tsne.tsne(soa_trainzeo[:2048, :1024])\n",
    "# plt.rcParams['figure.figsize'] = (15, 8)\n",
    "# plt.scatter(coor_tsne[:, 0], coor_tsne[:, 1], c=soa_trainy[:2048], marker=\"o\")\n",
    "\n",
    "# for i, txt in enumerate(coor_tsne):\n",
    "#     plt.annotate(soa_trainy[i], (txt[0],txt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8192)\n",
      "(None, 100)\n",
      "(None, 10)\n"
     ]
    }
   ],
   "source": [
    "from lasagne.layers import Conv2DLayer, InputLayer, ConcatLayer, DenseLayer, Pool2DLayer, FlattenLayer\n",
    "\n",
    "input_var = T.matrix('inputs')\n",
    "\n",
    "input_shape = (None, 8192)\n",
    "net = net_in = InputLayer(shape=input_shape, input_var=input_var)\n",
    "print net.output_shape\n",
    "\n",
    "# net = lasagne.layers.batch_norm(DenseLayer(net, num_units=100))\n",
    "# print net.output_shape\n",
    "\n",
    "# net = lasagne.layers.batch_norm(DenseLayer(net, num_units=100))\n",
    "# print net.output_shape\n",
    "\n",
    "# net = lasagne.layers.batch_norm(DenseLayer(net, num_units=100))\n",
    "# print net.output_shape\n",
    "\n",
    "net = lasagne.layers.batch_norm(DenseLayer(net, num_units=100))\n",
    "print net.output_shape\n",
    "\n",
    "#net = ConcatLayer([net_1,net_2])\n",
    "\n",
    "net = DenseLayer(net, num_units=10, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "print net.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: (None, 8192) -> output_shape: (None, 10)\n"
     ]
    }
   ],
   "source": [
    "output_shape = lasagne.layers.get_output_shape(net)\n",
    "print \"input_shape:\",input_shape,\"-> output_shape:\",output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = lasagne.layers.get_output(net)\n",
    "y = T.argmax(output, axis=1)\n",
    "f_predict = theano.function([input_var], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE building training functions\n"
     ]
    }
   ],
   "source": [
    "#Training function\n",
    "target_var = T.ivector('target')\n",
    "target_one_hot = T.extra_ops.to_one_hot(target_var, 10)\n",
    "loss = lasagne.objectives.categorical_crossentropy(output, target_one_hot)\n",
    "loss = loss.mean()\n",
    "\n",
    "#create lr variable so we can adjust the learning rate without compiling\n",
    "lr = theano.shared(np.array(0.001, dtype=theano.config.floatX))\n",
    "\n",
    "params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "updates = lasagne.updates.rmsprop(loss, params, learning_rate=lr)\n",
    "#updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=lr, momentum=0.9)\n",
    "\n",
    "f_train = theano.function([input_var, target_var], loss, updates=updates)\n",
    "print \"DONE building training functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "iteration: 0, trainerror: 2.12699, validacc: 0.46753, testacc: 0.46835, seconds: 0\n",
      "iteration: 5, trainerror: 0.25929, validacc: 0.60767, testacc: 0.61584, seconds: 0\n",
      "iteration: 10, trainerror: 0.03115, validacc: 0.64160, testacc: 0.65105, seconds: 0\n",
      "iteration: 15, trainerror: 0.00504, validacc: 0.64954, testacc: 0.66536, seconds: 0\n",
      "iteration: 20, trainerror: 0.00230, validacc: 0.65234, testacc: 0.66815, seconds: 0\n",
      "iteration: 25, trainerror: 0.00139, validacc: 0.65442, testacc: 0.66920, seconds: 0\n",
      "iteration: 30, trainerror: 0.00115, validacc: 0.65381, testacc: 0.66906, seconds: 0\n",
      "iteration: 35, trainerror: 0.00086, validacc: 0.65491, testacc: 0.66918, seconds: 0\n",
      "iteration: 40, trainerror: 0.00061, validacc: 0.65625, testacc: 0.67073, seconds: 0\n",
      "iteration: 45, trainerror: 0.00049, validacc: 0.65601, testacc: 0.67054, seconds: 0\n",
      "iteration: 50, trainerror: 0.00049, validacc: 0.65674, testacc: 0.67103, seconds: 0\n",
      "iteration: 55, trainerror: 0.00039, validacc: 0.65491, testacc: 0.67059, seconds: 0\n",
      "iteration: 60, trainerror: 0.00036, validacc: 0.65784, testacc: 0.67110, seconds: 0\n",
      "iteration: 65, trainerror: 0.00036, validacc: 0.65747, testacc: 0.67094, seconds: 0\n",
      "iteration: 70, trainerror: 0.00032, validacc: 0.65698, testacc: 0.67089, seconds: 0\n",
      "iteration: 75, trainerror: 0.00024, validacc: 0.65759, testacc: 0.67155, seconds: 0\n",
      "iteration: 80, trainerror: 0.00027, validacc: 0.65674, testacc: 0.67169, seconds: 0\n",
      "iteration: 85, trainerror: 0.00022, validacc: 0.65686, testacc: 0.67094, seconds: 0\n",
      "iteration: 90, trainerror: 0.00024, validacc: 0.65784, testacc: 0.67150, seconds: 0\n",
      "iteration: 95, trainerror: 0.00018, validacc: 0.65784, testacc: 0.67157, seconds: 0\n",
      "iteration: 100, trainerror: 0.00018, validacc: 0.65723, testacc: 0.67148, seconds: 0\n",
      "iteration: 105, trainerror: 0.00019, validacc: 0.65735, testacc: 0.67145, seconds: 0\n",
      "iteration: 110, trainerror: 0.00015, validacc: 0.65723, testacc: 0.67127, seconds: 0\n",
      "iteration: 115, trainerror: 0.00015, validacc: 0.65808, testacc: 0.67122, seconds: 0\n",
      "iteration: 120, trainerror: 0.00015, validacc: 0.65649, testacc: 0.67148, seconds: 0\n",
      "iteration: 125, trainerror: 0.00014, validacc: 0.65674, testacc: 0.67117, seconds: 0\n",
      "iteration: 130, trainerror: 0.00014, validacc: 0.65735, testacc: 0.67127, seconds: 0\n",
      "iteration: 135, trainerror: 0.00012, validacc: 0.65723, testacc: 0.67120, seconds: 0\n",
      "iteration: 140, trainerror: 0.00010, validacc: 0.65735, testacc: 0.67159, seconds: 0\n",
      "iteration: 145, trainerror: 0.00012, validacc: 0.65759, testacc: 0.67101, seconds: 0\n",
      "iteration: 150, trainerror: 0.00012, validacc: 0.65686, testacc: 0.67127, seconds: 0\n",
      "iteration: 155, trainerror: 0.00010, validacc: 0.65710, testacc: 0.67110, seconds: 0\n",
      "iteration: 160, trainerror: 0.00010, validacc: 0.65686, testacc: 0.67173, seconds: 0\n",
      "iteration: 165, trainerror: 0.00010, validacc: 0.65771, testacc: 0.67162, seconds: 0\n",
      "iteration: 170, trainerror: 0.00009, validacc: 0.65808, testacc: 0.67124, seconds: 0\n",
      "iteration: 175, trainerror: 0.00009, validacc: 0.65906, testacc: 0.67176, seconds: 0\n",
      "iteration: 180, trainerror: 0.00009, validacc: 0.65796, testacc: 0.67145, seconds: 0\n",
      "iteration: 185, trainerror: 0.00008, validacc: 0.65820, testacc: 0.67115, seconds: 0\n",
      "iteration: 190, trainerror: 0.00008, validacc: 0.65735, testacc: 0.67127, seconds: 0\n",
      "iteration: 195, trainerror: 0.00008, validacc: 0.65820, testacc: 0.67120, seconds: 0\n",
      "iteration: 200, trainerror: 0.00008, validacc: 0.65735, testacc: 0.67176, seconds: 0\n",
      "iteration: 205, trainerror: 0.00007, validacc: 0.65833, testacc: 0.67148, seconds: 0\n",
      "iteration: 210, trainerror: 0.00008, validacc: 0.65784, testacc: 0.67150, seconds: 0\n",
      "iteration: 215, trainerror: 0.00007, validacc: 0.65796, testacc: 0.67129, seconds: 0\n",
      "iteration: 220, trainerror: 0.00008, validacc: 0.65808, testacc: 0.67155, seconds: 0\n",
      "iteration: 225, trainerror: 0.00007, validacc: 0.65796, testacc: 0.67143, seconds: 0\n",
      "iteration: 230, trainerror: 0.00006, validacc: 0.65894, testacc: 0.67162, seconds: 0\n",
      "iteration: 235, trainerror: 0.00008, validacc: 0.65894, testacc: 0.67157, seconds: 0\n",
      "iteration: 240, trainerror: 0.00006, validacc: 0.65771, testacc: 0.67155, seconds: 0\n",
      "iteration: 245, trainerror: 0.00007, validacc: 0.65808, testacc: 0.67150, seconds: 0\n",
      "iteration: 250, trainerror: 0.00007, validacc: 0.65784, testacc: 0.67164, seconds: 0\n",
      "iteration: 255, trainerror: 0.00006, validacc: 0.65820, testacc: 0.67185, seconds: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-255-fe26ee7baf8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoa_testx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m#len(soa_testx)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mx_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoa_testzeo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mtestacc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msoa_testy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/u/cohenjos/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "# train model\n",
    "batch_size = 64\n",
    "print \"batch_size:\",batch_size\n",
    "\n",
    "stats = []\n",
    "for i in range(0,2000):\n",
    "    start_time = time.time()\n",
    "    trainerror = []\n",
    "    \n",
    "    #Shuffle batches!\n",
    "    order = range(0,len(soa_trainx))\n",
    "    random.shuffle(order)\n",
    "    z_train = soa_trainzeo[order]\n",
    "    t_train = soa_trainy[order]\n",
    "    \n",
    "    #Start batch training!\n",
    "    for start in range(0, len(soa_trainx), batch_size):\n",
    "        \n",
    "        z_batch = z_train[start:start + batch_size]\n",
    "        t_batch = t_train[start:start + batch_size]\n",
    "    \n",
    "        #z_batch = np.random.normal(z_batch, scale=0.0001).astype(np.float32)\n",
    "\n",
    "        \n",
    "        cost = f_train(z_batch, t_batch)\n",
    "        \n",
    "        trainerror.append(cost)\n",
    "        #print \"batch error: %.5f\" % cost\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        ## Calculate valid and test error \n",
    "        pred = np.array([])\n",
    "        for start in range(0, len(soa_validx), batch_size):   #len(soa_testx)\n",
    "            x_batch = soa_validzeo[start:start + batch_size]\n",
    "            pred = np.append(pred,f_predict(x_batch))\n",
    "        validacc = np.mean(pred == soa_validy)\n",
    "\n",
    "        pred = np.array([])\n",
    "        for start in range(0, len(soa_testx), batch_size):   #len(soa_testx)\n",
    "            x_batch = soa_testzeo[start:start + batch_size]\n",
    "            pred = np.append(pred,f_predict(x_batch))\n",
    "        testacc = np.mean(pred == soa_testy[:len(pred)])\n",
    "\n",
    "        epocherror = np.mean(trainerror)\n",
    "        stats.append((validacc,epocherror, testacc))\n",
    "\n",
    "        print \"iteration: %d, trainerror: %.5f, validacc: %.5f, testacc: %.5f, seconds: %d\" % (i, epocherror,validacc, testacc, elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = np.array([])\n",
    "for start in range(0, len(soa_trainzeo), batch_size):   #len(soa_testx)\n",
    "    zeo_batch = soa_trainzeo[start:start + batch_size]\n",
    "    pred = np.append(pred,f_predict(zeo_batch))\n",
    "trainacc = np.mean(pred == soa_trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = np.array([])\n",
    "for start in range(0, len(soa_testzeo), batch_size):   #len(soa_testx)\n",
    "    zeo_batch = soa_testzeo[start:start + batch_size]\n",
    "    pred = np.append(pred,f_predict(zeo_batch))\n",
    "testacc = np.mean(pred == soa_testy[:len(soa_testzeo)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.671664167916042"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
