{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5105 on context None\n",
      "Mapped name None to device cuda0: Quadro K6000 (0000:04:00.0)\n",
      "/Tmp/lisa/os_v5/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python \n",
    "\n",
    "'''\n",
    "-Initially make z_to_x and x_to_z fairly shallow networks.  Inject noise?  \n",
    "\n",
    "-Use the fflayer class?  \n",
    "\n",
    "'''\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/u/lambalex/DeepLearning/undirected_matching\")\n",
    "sys.path.append(\"/u/lambalex/DeepLearning/undirected_matching/lib\")\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from nn_layers import fflayer, param_init_fflayer, param_init_convlayer, convlayer\n",
    "from utils import init_tparams, join2, srng, dropout, inverse_sigmoid, join3, merge_images\n",
    "from loss import accuracy, crossent, lsgan_loss, wgan_loss, improvement_loss\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "import random\n",
    "from viz import plot_images\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "slurm_name = os.environ[\"SLURM_JOB_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (574168, 3, 32, 32)\n",
      "svhn shape (574168, 3, 32, 32)\n",
      "0 255\n",
      "num latent 128\n",
      "dataset svhn\n",
      "num steps 1\n",
      "train classifier separate True\n",
      "latent sparse False\n",
      "persistent p chain False\n",
      "blending rate (odds of keeping old z in P chain) 0.5\n",
      "improvement loss weight 0.0\n"
     ]
    }
   ],
   "source": [
    "class ConsiderConstant(theano.compile.ViewOp):\n",
    "    def grad(self, args, g_outs):\n",
    "        return [T.zeros_like(g_out) for g_out in g_outs]\n",
    "\n",
    "consider_constant = ConsiderConstant()\n",
    "\n",
    "#dataset = \"mnist\"\n",
    "#dataset = \"anime\"\n",
    "dataset = \"svhn\"\n",
    "\n",
    "if dataset == \"mnist\":\n",
    "    mn = gzip.open(\"/u/lambalex/data/mnist/mnist.pkl.gz\")\n",
    "\n",
    "    train, valid, test = pickle.load(mn)\n",
    "\n",
    "    trainx,trainy = train\n",
    "    \n",
    "    \n",
    "    #newtx = trainx[(trainy<2) | (trainy>8)]\n",
    "    #newty = trainy[(trainy<2) | (trainy>8)]\n",
    "    #trainx = newtx\n",
    "    #trainy = newty\n",
    "    \n",
    "    validx,validy = valid\n",
    "    testx, testy = test\n",
    "\n",
    "    num_examples = trainx.shape[0]\n",
    "\n",
    "    m = 784\n",
    "elif dataset == \"anime\":\n",
    "    from load_file import FileData, normalize, denormalize\n",
    "\n",
    "    loc = \"/u/lambalex/DeepLearning/animefaces/datafaces/danbooru-faces/\"\n",
    "\n",
    "    animeData = FileData(loc, 32, 64)\n",
    "\n",
    "    m = 32*32*3\n",
    "\n",
    "elif dataset == \"svhn\":\n",
    "\n",
    "    from load_svhn import SvhnData\n",
    "    from load_file import normalize, denormalize\n",
    "\n",
    "    svhnData = SvhnData(mb_size=64,segment=\"train\")\n",
    "\n",
    "    num_examples = 50000\n",
    "\n",
    "nl = 128\n",
    "print \"num latent\", nl\n",
    "#128 works for nl\n",
    "nfg = 512\n",
    "nfd = 512\n",
    "\n",
    "print \"dataset\", dataset\n",
    "\n",
    "#3\n",
    "num_steps = 1\n",
    "print \"num steps\", num_steps\n",
    "\n",
    "train_classifier_separate = True\n",
    "print \"train classifier separate\", train_classifier_separate\n",
    "\n",
    "#skip_conn = True\n",
    "#print \"skip conn\", skip_conn\n",
    "\n",
    "latent_sparse = False\n",
    "print \"latent sparse\", latent_sparse\n",
    "\n",
    "persist_p_chain = False\n",
    "print \"persistent p chain\", persist_p_chain\n",
    "\n",
    "blending_rate = 0.5\n",
    "print 'blending rate (odds of keeping old z in P chain)', blending_rate\n",
    "\n",
    "improvement_loss_weight = 0.0\n",
    "print \"improvement loss weight\", improvement_loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_gparams(p):\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='z_x_1',nin=nl*2,nout=512*4*4,ortho=False,batch_norm=True)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_2',nin=512,nout=256,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_3',nin=256*1,nout=128,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='z_x_4',nin=128*1,nout=3,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_1',nin=3,nout=128,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_2',nin=128,nout=256,kernel_len=5,batch_norm=True)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='x_z_3',nin=256,nout=512,kernel_len=5,batch_norm=True)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_mu',nin=512*4*4,nout=nl,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='x_z_sigma',nin=512*4*4,nout=nl,ortho=False,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "def init_dparams(p):\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_1',nin=3,nout=128,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_2',nin=128,nout=256,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='DC_3',nin=256,nout=512,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_1',nin=nl+512*4*4,nout=nfd,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_2',nin=nfd,nout=nfd,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_3',nin=nfd,nout=nfd,ortho=False,batch_norm=False)\n",
    "\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_1',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_2',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "    p = param_init_fflayer(options={},params=p,prefix='D_o_3',nin=nfd,nout=1,ortho=False,batch_norm=False)\n",
    "\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_4',nin=128,nout=1,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_5',nin=256,nout=1,kernel_len=5,batch_norm=False)\n",
    "    p = param_init_convlayer(options={},params=p,prefix='D_o_6',nin=512,nout=1,kernel_len=5,batch_norm=False)\n",
    "\n",
    "    return init_tparams(p)\n",
    "\n",
    "\n",
    "def z_to_x(p,z):\n",
    "\n",
    "    print \"extra noise input\"\n",
    "    z_inp = join2(z, 1.0*srng.normal(size=z.shape))\n",
    "\n",
    "    d0 = fflayer(tparams=p,state_below=z_inp,options={},prefix='z_x_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)')\n",
    "\n",
    "    d0 = d0.reshape((64,512,4,4))\n",
    "\n",
    "    d1 = convlayer(tparams=p,state_below=d0,options={},prefix='z_x_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=-2)\n",
    "\n",
    "    d2 = convlayer(tparams=p,state_below=d1,options={},prefix='z_x_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=-2)\n",
    "\n",
    "    d3 = convlayer(tparams=p,state_below=d2,options={},prefix='z_x_4',activ='lambda x: x',stride=-2)\n",
    "\n",
    "    x_new = d3.flatten(2)\n",
    "\n",
    "    return x_new\n",
    "\n",
    "def x_to_z(p,x):\n",
    "\n",
    "    e1 = convlayer(tparams=p,state_below=x.reshape((64,3,32,32)),options={},prefix='x_z_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    e2 = convlayer(tparams=p,state_below=e1,options={},prefix='x_z_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    e3 = convlayer(tparams=p,state_below=e2,options={},prefix='x_z_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    eo = e3\n",
    "    eo = eo.flatten(2)\n",
    "\n",
    "    sigma = fflayer(tparams=p,state_below=eo,options={},prefix='x_z_mu',activ='lambda x: x')\n",
    "    mu = fflayer(tparams=p,state_below=eo,options={},prefix='x_z_sigma',activ='lambda x: x')\n",
    "\n",
    "    eps = srng.normal(size=sigma.shape)\n",
    "\n",
    "    z_new = eps*T.nnet.sigmoid(sigma) + mu\n",
    "    print \"turned on injected noise in x->z connection\"\n",
    "\n",
    "    z_new = (z_new - T.mean(z_new, axis=0, keepdims=True)) / (0.001 + T.std(z_new, axis=0, keepdims=True))\n",
    "\n",
    "    return z_new\n",
    "\n",
    "\n",
    "def discriminator(p,x,z):\n",
    "\n",
    "    dc_1 = convlayer(tparams=p,state_below=x.reshape((64,3,32,32)),options={},prefix='DC_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    dc_2 = convlayer(tparams=p,state_below=dc_1,options={},prefix='DC_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    dc_3 = convlayer(tparams=p,state_below=dc_2,options={},prefix='DC_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',stride=2)\n",
    "\n",
    "    inp = join2(z,dc_3.flatten(2))\n",
    "\n",
    "    h1 = fflayer(tparams=p,state_below=inp,options={},prefix='D_1',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)',mean_ln=False)\n",
    "\n",
    "    h2 = fflayer(tparams=p,state_below=h1,options={},prefix='D_2',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)', mean_ln=False)\n",
    "\n",
    "    h3 = fflayer(tparams=p,state_below=h2,options={},prefix='D_3',activ='lambda x: tensor.nnet.relu(x,alpha=0.02)', mean_ln=False)\n",
    "\n",
    "    D1 = fflayer(tparams=p,state_below=h1,options={},prefix='D_o_1',activ='lambda x: x')\n",
    "    D2 = fflayer(tparams=p,state_below=h2,options={},prefix='D_o_2',activ='lambda x: x')\n",
    "    D3 = fflayer(tparams=p,state_below=h3,options={},prefix='D_o_3',activ='lambda x: x')\n",
    "\n",
    "    D4 = convlayer(tparams=p,state_below=dc_1,options={},prefix='D_o_4',activ='lambda x: x',stride=2)\n",
    "    D5 = convlayer(tparams=p,state_below=dc_2,options={},prefix='D_o_5',activ='lambda x: x',stride=2)\n",
    "    D6 = convlayer(tparams=p,state_below=dc_3,options={},prefix='D_o_6',activ='lambda x: x',stride=2)\n",
    "\n",
    "    print \"special thing in D\"\n",
    "    return [D1,D2,D3,D4,D5,D6], h3\n",
    "\n",
    "def p_chain(p, z, num_iterations):\n",
    "    zlst = [z]\n",
    "    xlst = []\n",
    "\n",
    "    if num_iterations == 1:\n",
    "        \n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        #new_z = x_to_z(p, xlst[-1])\n",
    "        #zlst.append(new_z)\n",
    "\n",
    "    elif num_iterations == 3:  \n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        new_z = x_to_z(p, consider_constant(xlst[-1]))\n",
    "        zlst.append(new_z)\n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "        new_z = x_to_z(p, consider_constant(xlst[-1]))\n",
    "        zlst.append(new_z)\n",
    "\n",
    "        new_x = z_to_x(p, zlst[-1])\n",
    "        xlst.append(new_x)\n",
    "\n",
    "    else:\n",
    "\n",
    "        for inds in range(0,num_iterations):\n",
    "            new_x = z_to_x(p, zlst[-1])\n",
    "            xlst.append(new_x)\n",
    "            new_z = x_to_z(p, xlst[-1])\n",
    "            zlst.append(new_z)\n",
    "\n",
    "\n",
    "    for j in range(len(xlst)):\n",
    "        xlst[j] = T.nnet.sigmoid(xlst[j])\n",
    "\n",
    "    return xlst, zlst\n",
    "\n",
    "def onestep_z_to_x(p,z):\n",
    "    x = T.nnet.sigmoid(z_to_x(p, z))\n",
    "    return x\n",
    "\n",
    "def onestep_x_to_z(p,x):\n",
    "    new_z = x_to_z(p, inverse_sigmoid(x))\n",
    "    return new_z\n",
    "\n",
    "def q_chain(p,x,num_iterations):\n",
    "\n",
    "    xlst = [x]\n",
    "    zlst = []\n",
    "    new_z = x_to_z(p, inverse_sigmoid(xlst[-1]))\n",
    "    zlst.append(new_z)\n",
    "\n",
    "    return xlst, zlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n",
      "[sigmoid.0]\n",
      "[z_in]\n",
      "[<TensorType(float32, matrix)>]\n",
      "[Elemwise{true_div,no_inplace}.0]\n",
      "special thing in D\n",
      "special thing in D\n",
      "single disc\n",
      "not using improvement objective\n",
      "extra noise input\n",
      "turned on injected noise in x->z connection\n"
     ]
    }
   ],
   "source": [
    "gparams = init_gparams({})\n",
    "dparams = init_dparams({})\n",
    "\n",
    "z_in = T.matrix('z_in')\n",
    "x_in = T.matrix()\n",
    "\n",
    "p_lst_x,p_lst_z = p_chain(gparams, z_in, num_steps)\n",
    "\n",
    "q_lst_x,q_lst_z = q_chain(gparams, x_in, num_steps)\n",
    "\n",
    "p_lst_x_long,p_lst_z_long = p_chain(gparams, z_in, 19)\n",
    "\n",
    "z_inf = q_lst_z[-1]\n",
    "\n",
    "print p_lst_x\n",
    "print p_lst_z\n",
    "print q_lst_x\n",
    "print q_lst_z\n",
    "\n",
    "#D_p_lst_3,_ = discriminator(dparams, p_lst_x[2], p_lst_z[2])\n",
    "\n",
    "#D_p_lst_2,_ = discriminator(dparams, p_lst_x[1], p_lst_z[1])\n",
    "\n",
    "D_p_lst_1,_ = discriminator(dparams, p_lst_x[0], p_lst_z[0])\n",
    "\n",
    "D_q_lst,D_feat_q = discriminator(dparams, q_lst_x[-1], q_lst_z[-1])\n",
    "\n",
    "dloss, gloss = lsgan_loss(D_q_lst, D_p_lst_1)\n",
    "\n",
    "print \"single disc\"\n",
    "print \"not using improvement objective\"\n",
    "#improvement_objective = improvement_loss_weight * improvement_loss(D_p_lst_1, D_p_lst_2)\n",
    "#gloss += improvement_objective\n",
    "\n",
    "lr = theano.shared(np.array(0.0001, dtype=theano.config.floatX))\n",
    "\n",
    "dupdates = lasagne.updates.rmsprop(dloss, dparams.values(),lr)\n",
    "gloss_grads = T.grad(gloss, gparams.values(), disconnected_inputs='ignore')\n",
    "gupdates = lasagne.updates.rmsprop(gloss_grads, gparams.values(),lr)\n",
    "\n",
    "gcupdates = lasagne.updates.rmsprop(gloss, gparams.values(),lr)\n",
    "\n",
    "dgupdates = dupdates.copy()\n",
    "dgupdates.update(gupdates)\n",
    "\n",
    "dgcupdates = dupdates.copy()\n",
    "dgcupdates.update(gcupdates)\n",
    "\n",
    "train_disc_gen_classifier = theano.function(inputs = [x_in, z_in], outputs=[dloss,p_lst_x[-1],p_lst_z[-1]], updates=dgcupdates,on_unused_input='ignore')\n",
    "\n",
    "train_disc_gen_classifier_fast = theano.function(inputs = [x_in, z_in], outputs=[dloss], updates=dgcupdates,on_unused_input='ignore')\n",
    "\n",
    "\n",
    "get_zinf = theano.function([x_in], outputs=z_inf)\n",
    "#get_dfeat = theano.function([x_in], outputs=D_feat_q)\n",
    "\n",
    "#get_pchain = theano.function([z_in], outputs = p_lst_x_long)\n",
    "\n",
    "x_in = T.matrix()\n",
    "\n",
    "func_z_to_x = theano.function([z_in], outputs = onestep_z_to_x(gparams, z_in))\n",
    "func_x_to_z = theano.function([x_in], outputs = onestep_x_to_z(gparams, x_in))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "directory = \"network-temp/\"\n",
    "ext = \"svhn-soa.p\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "def save_network(gparams,dparams,name):\n",
    "    pkl_params = (gparams,dparams)\n",
    "    out = open(directory + str(name) + ext, \"w\", 0) #bufsize=0\n",
    "    pickle.dump(pkl_params, out)\n",
    "    out.close()\n",
    "\n",
    "def load_network(name):\n",
    "    return pickle.load(open(directory + str(name) + ext, \"r\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr.set_value(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 dloss [array(0.5324965715408325, dtype=float32)] gen_x mean 0.476226\n",
      "dloss [array(0.5324965715408325, dtype=float32)]\n",
      "iteration 10 dloss [array(0.36592480540275574, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 20 dloss [array(0.4246293902397156, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 30 dloss [array(0.4873059391975403, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 40 dloss [array(0.4722258746623993, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 50 dloss [array(0.49683910608291626, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 60 dloss [array(0.5637909173965454, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 70 dloss [array(0.4539121985435486, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 80 dloss [array(0.48792609572410583, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 90 dloss [array(0.4955250918865204, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 100 dloss [array(0.4334373474121094, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 110 dloss [array(0.3613569140434265, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 120 dloss [array(0.48754554986953735, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 130 dloss [array(0.483096182346344, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 140 dloss [array(0.4764025807380676, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 150 dloss [array(0.5041280388832092, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 160 dloss [array(0.540774941444397, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 170 dloss [array(0.46558111906051636, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 180 dloss [array(0.4833691716194153, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 190 dloss [array(0.4606878459453583, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 200 dloss [array(0.4730357527732849, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 210 dloss [array(0.43470364809036255, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 220 dloss [array(0.4730610251426697, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 230 dloss [array(0.429391473531723, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 240 dloss [array(0.5210113525390625, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 250 dloss [array(0.5510802268981934, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 260 dloss [array(0.44121336936950684, dtype=float32)] gen_x mean 0.436229\n",
      "iteration 270 dloss [array(0.5065817832946777, dtype=float32)] gen_x mean 0.436229\n"
     ]
    }
   ],
   "source": [
    "z_out_p = rng.normal(size=(64,nl)).astype('float32')\n",
    "\n",
    "for iteration in range(0,500000):\n",
    "\n",
    "    if persist_p_chain:\n",
    "        z_in_new = rng.normal(size=(64,nl)).astype('float32')\n",
    "        blending = rng.uniform(0.0,1.0,size=(64,))\n",
    "        z_in_new[blending>=blending_rate] = z_out_p[blending>=blending_rate]\n",
    "        z_in = z_in_new\n",
    "    else:\n",
    "        z_in = rng.normal(size=(64,nl)).astype('float32')\n",
    "\n",
    "    if latent_sparse:\n",
    "        z_in[:,128:] *= 0.0\n",
    "\n",
    "    r = random.randint(0,num_examples-64)\n",
    "\n",
    "    if dataset == \"mnist\":\n",
    "        x_in = trainx[r:r+64]\n",
    "        y_in = trainy[r:r+64]\n",
    "\n",
    "        x_in = x_in.reshape((64,1,28,28))\n",
    "\n",
    "        x_in = np.repeat(x_in,3,axis=(1))\n",
    "        x_in = np.lib.pad(x_in,((0,0),(0,0),(2,2),(2,2)),'constant',constant_values=(0))\n",
    "\n",
    "        x_in = x_in.reshape((64,32*32*3))\n",
    "    elif dataset == \"anime\":\n",
    "        x_in = normalize(animeData.getBatch()).reshape((64,32*32*3))\n",
    "\n",
    "    elif dataset == \"svhn\":\n",
    "        x_in = normalize(svhnData.getBatch()['x']).reshape((64,32*32*3))\n",
    "\n",
    "    #dloss,gen_x,z_out_p = train_disc_gen_classifier(x_in,z_in)\n",
    "    \n",
    "    dloss= train_disc_gen_classifier_fast(x_in,z_in)\n",
    "\n",
    "\n",
    "    if iteration % 10 == 0:\n",
    "        print \"iteration\", iteration, \"dloss\", dloss, \"gen_x mean\", gen_x.mean()\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        print \"dloss\", dloss\n",
    "        dloss,gen_x,z_out_p = train_disc_gen_classifier(x_in,z_in)\n",
    "        plot_images(gen_x, \"plots/\" + slurm_name + \"_gen.png\")\n",
    "        \n",
    "        \n",
    "        save_network(gparams,dparams,\"test\")\n",
    "        \n",
    "        #plot_images(reconstruct(x_in).reshape((64,1,28,28)), \"plots/\" + slurm_name + \"_rec.png\")\n",
    "\n",
    "        #NOT CORRECT INITIALLY\n",
    "        #rec_loop = [x_in]\n",
    "        #for b in range(0,9):\n",
    "        #    rec_loop.append(reconstruct(rec_loop[-1]))\n",
    "        #    rec_loop[-1][:,0:392] = x_in[:,0:392]\n",
    "        #    plot_images(rec_loop[-1].reshape((64,1,28,28)), \"plots/\" + slurm_name + \"_rec_\" + str(b) +\".png\")\n",
    "\n",
    "#         plot_images(x_in, \"plots/\" + slurm_name + \"_original.png\")\n",
    "\n",
    "#         #p_chain = get_pchain(z_in)\n",
    "#         new_z = rng.normal(size=(64,nl)).astype('float32')\n",
    "#         for j in range(0,20):\n",
    "#             new_x = func_z_to_x(new_z)\n",
    "#             new_z = func_x_to_z(new_x)\n",
    "#             print \"printing element of p_chain\", j\n",
    "#             plot_images(new_x, \"plots/\" + slurm_name + \"_pchain_\" + str(j) + \".png\")\n",
    "\n",
    "#         new_z = rng.normal(size=(64,nl)).astype('float32')\n",
    "#         for j in range(0,20):\n",
    "#             new_x = func_z_to_x(new_z)\n",
    "#             new_x = merge_images(new_x, x_in)\n",
    "#             new_z = func_x_to_z(new_x)\n",
    "#             plot_images(new_x, \"plots/\" + slurm_name + \"_inpainting_\" + str(j) + \".png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_network(gparams,dparams,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('x_z_3_newsigma', x_z_3_newsigma),\n",
       "              ('x_z_2_newsigma', x_z_2_newsigma),\n",
       "              ('x_z_mu_W', x_z_mu_W),\n",
       "              ('z_x_2_newsigma', z_x_2_newsigma),\n",
       "              ('z_x_3_b', z_x_3_b),\n",
       "              ('x_z_1_b', x_z_1_b),\n",
       "              ('x_z_sigma_W', x_z_sigma_W),\n",
       "              ('x_z_1_newsigma', x_z_1_newsigma),\n",
       "              ('z_x_2_b', z_x_2_b),\n",
       "              ('z_x_1_newmu', z_x_1_newmu),\n",
       "              ('x_z_2_W', x_z_2_W),\n",
       "              ('z_x_4_b', z_x_4_b),\n",
       "              ('z_x_3_newmu', z_x_3_newmu),\n",
       "              ('z_x_2_W', z_x_2_W),\n",
       "              ('x_z_2_b', x_z_2_b),\n",
       "              ('z_x_4_W', z_x_4_W),\n",
       "              ('z_x_1_newsigma', z_x_1_newsigma),\n",
       "              ('z_x_3_newsigma', z_x_3_newsigma),\n",
       "              ('x_z_3_newmu', x_z_3_newmu),\n",
       "              ('z_x_3_W', z_x_3_W),\n",
       "              ('x_z_1_W', x_z_1_W),\n",
       "              ('z_x_1_W', z_x_1_W),\n",
       "              ('x_z_sigma_b', x_z_sigma_b),\n",
       "              ('x_z_1_newmu', x_z_1_newmu),\n",
       "              ('x_z_2_newmu', x_z_2_newmu),\n",
       "              ('x_z_3_b', x_z_3_b),\n",
       "              ('x_z_3_W', x_z_3_W),\n",
       "              ('x_z_mu_b', x_z_mu_b),\n",
       "              ('z_x_2_newmu', z_x_2_newmu),\n",
       "              ('z_x_1_b', z_x_1_b)]),\n",
       " OrderedDict([('D_o_2_W', D_o_2_W),\n",
       "              ('DC_3_W', DC_3_W),\n",
       "              ('D_1_W', D_1_W),\n",
       "              ('DC_2_W', DC_2_W),\n",
       "              ('D_o_3_W', D_o_3_W),\n",
       "              ('D_o_5_b', D_o_5_b),\n",
       "              ('D_3_W', D_3_W),\n",
       "              ('D_o_1_W', D_o_1_W),\n",
       "              ('D_2_b', D_2_b),\n",
       "              ('D_o_1_b', D_o_1_b),\n",
       "              ('D_3_b', D_3_b),\n",
       "              ('D_2_W', D_2_W),\n",
       "              ('D_o_4_b', D_o_4_b),\n",
       "              ('D_1_b', D_1_b),\n",
       "              ('D_o_4_W', D_o_4_W),\n",
       "              ('DC_3_b', DC_3_b),\n",
       "              ('D_o_2_b', D_o_2_b),\n",
       "              ('D_o_5_W', D_o_5_W),\n",
       "              ('D_o_3_b', D_o_3_b),\n",
       "              ('DC_2_b', DC_2_b),\n",
       "              ('D_o_6_W', D_o_6_W),\n",
       "              ('DC_1_b', DC_1_b),\n",
       "              ('D_o_6_b', D_o_6_b),\n",
       "              ('DC_1_W', DC_1_W)]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load_network(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svhnData.getBatch()['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svhnData.train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soa_trainx = svhnData.train_X[:1024]\n",
    "soa_trainy = svhnData.train_Y[:1024].reshape(1024,)\n",
    "\n",
    "\n",
    "soa_testx = svhnData.train_X[1024:]\n",
    "soa_testy = svhnData.train_Y[1024:].reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soa_trainx.shape\n",
    "print soa_trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soa_testx.shape\n",
    "print soa_testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "x_under_test = normalize(soa_trainx[:64]).reshape((64,32*32*3))\n",
    "\n",
    "z_under_test = func_x_to_z(x_under_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_under_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z_under_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(z_under_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import Conv2DLayer, InputLayer, ConcatLayer, DenseLayer, Pool2DLayer, FlattenLayer\n",
    "\n",
    "input_var = T.matrix('inputs')\n",
    "\n",
    "input_shape = (None, 128)\n",
    "net = InputLayer(shape=input_shape, input_var=input_var)\n",
    "\n",
    "net = lasagne.layers.batch_norm(DenseLayer(net, num_units=1000))\n",
    "print net.output_shape\n",
    "\n",
    "net = lasagne.layers.batch_norm(DenseLayer(net, num_units=1000))\n",
    "print net.output_shape\n",
    "\n",
    "net = DenseLayer(net, num_units=10, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "print net.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_shape = lasagne.layers.get_output_shape(net)\n",
    "print \"input_shape:\",input_shape,\"-> output_shape:\",output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = lasagne.layers.get_output(net)\n",
    "y = T.argmax(output, axis=1)\n",
    "f_predict = theano.function([input_var], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training function\n",
    "target_var = T.ivector('target')\n",
    "target_one_hot = T.extra_ops.to_one_hot(target_var, 10)\n",
    "loss = lasagne.objectives.categorical_crossentropy(output, target_one_hot)\n",
    "loss = loss.mean()\n",
    "\n",
    "#create lr variable so we can adjust the learning rate without compiling\n",
    "lr = theano.shared(np.array(0.001, dtype=theano.config.floatX))\n",
    "\n",
    "params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "updates = lasagne.updates.adam(loss, params, learning_rate=lr)\n",
    "#updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=lr, momentum=0.9)\n",
    "\n",
    "f_train = theano.function([input_var, target_var], loss, updates=updates)\n",
    "print \"DONE building training functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# train model\n",
    "batch_size = 64\n",
    "print \"batch_size:\",batch_size\n",
    "\n",
    "stats = []\n",
    "for i in range(0,200):\n",
    "    start_time = time.time()\n",
    "    trainerror = []\n",
    "\n",
    "    #Shuffle batches!\n",
    "    order = range(0,len(soa_trainx))\n",
    "    random.shuffle(order)\n",
    "    x_train = soa_trainx[order]\n",
    "    t_train = soa_trainy[order]\n",
    "    \n",
    "    #Start batch training!\n",
    "    for start in range(0, len(soa_trainx), batch_size):\n",
    "        \n",
    "        x_batch = x_train[start:start + batch_size]\n",
    "        t_batch = t_train[start:start + batch_size]\n",
    "        \n",
    "        x_under_test = normalize(x_batch).reshape((64,32*32*3))\n",
    "        z_under_test = func_x_to_z(x_under_test)\n",
    "        \n",
    "    \n",
    "        cost = f_train(z_under_test, t_batch)\n",
    "        \n",
    "        trainerror.append(cost)\n",
    "        #print \"batch error: %.5f\" % cost\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    ## Calculate test error (just for logging)\n",
    "    pred = np.array([])\n",
    "    for start in range(0, batch_size*100, batch_size):   #len(soa_testx)\n",
    "        x_batch = soa_testx[start:start + batch_size]\n",
    "        x_under_test = normalize(x_batch).reshape((64,32*32*3))\n",
    "        z_under_test = func_x_to_z(x_under_test)\n",
    "        pred = np.append(pred,f_predict(z_under_test))\n",
    "    testacc = np.mean(pred == soa_testy[:batch_size*100])\n",
    "    \n",
    "    epocherror = np.mean(trainerror)\n",
    "    stats.append((testacc,epocherror))\n",
    "    \n",
    "    print \"iteration: %d, trainerror: %.5f, accuracy: %.5f, seconds: %d\" % (i, epocherror,testacc, elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = np.array([])\n",
    "for start in range(0, batch_size*100, batch_size):   #len(soa_testx)\n",
    "    x_batch = soa_testx[start:start + batch_size]\n",
    "    x_under_test = normalize(x_batch).reshape((64,32*32*3))\n",
    "    z_under_test = func_x_to_z(x_under_test)\n",
    "    pred = np.append(pred,f_predict(z_under_test))\n",
    "testacc = np.mean(pred == soa_testy[:batch_size*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
